{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0f912a",
   "metadata": {},
   "source": [
    "## 1. check corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0799f194",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kiancai/STA24/CWD/STAi/MiCoGPT/data/try2_withCC/abundance_all_90338.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 123\u001b[0m\n\u001b[1;32m    119\u001b[0m         show_sample(idx)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# 1. 读入 corpus 对象\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCORPUS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     15\u001b[0m         corpus \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     17\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m corpus\u001b[38;5;241m.\u001b[39mtokens  \u001b[38;5;66;03m# shape: (num_samples, max_len)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/micromamba/envs/MiCoSeq_dev/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/kiancai/STA24/CWD/STAi/MiCoGPT/data/try2_withCC/abundance_all_90338.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "# ======= 手动改这里：你的 corpus.pkl 路径 =======\n",
    "CORPUS_PATH = \"/Users/kiancai/STA24/CWD/STAi/MiCoGPT/data/try2_withCC/abundance_all_90338.pkl\"\n",
    "\n",
    "# 想重点查看的样本 index（可以按需要改）\n",
    "CHECK_INDICES = [0, 1, 2]  # 例如前几个样本\n",
    "WINDOW = 5  # 看 eos 前后各多少个 token\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1. 读入 corpus 对象\n",
    "    with open(CORPUS_PATH, \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "\n",
    "    tokens = corpus.tokens  # shape: (num_samples, max_len)\n",
    "    pad_id = corpus.tokenizer.pad_token_id\n",
    "    eos_id = getattr(corpus.tokenizer, \"eos_token_id\", None)\n",
    "    bos_id = getattr(corpus.tokenizer, \"bos_token_id\", None)\n",
    "\n",
    "    print(f\"Loaded corpus from: {CORPUS_PATH}\")\n",
    "    print(f\"tokens.shape = {tokens.shape}\")\n",
    "    print(f\"pad_token_id = {pad_id}\")\n",
    "    print(f\"bos_token_id = {bos_id}\")\n",
    "    print(f\"eos_token_id = {eos_id}\")\n",
    "\n",
    "    if eos_id is None:\n",
    "        print(\"⚠ tokenizer 没有 eos_token_id 属性，先确认 tokenizer 里是怎么定义 eos 的。\")\n",
    "        return\n",
    "\n",
    "    # 2. 计算每个样本的“有效长度”（非 pad token 的数量）\n",
    "    non_pad_mask = (tokens != pad_id)\n",
    "    lengths = non_pad_mask.sum(dim=1)  # (num_samples,)\n",
    "    max_len_val = int(lengths.max())\n",
    "    min_len_val = int(lengths.min())\n",
    "    idx_max = int(lengths.argmax())\n",
    "    idx_min = int(lengths.argmin())\n",
    "\n",
    "    print(\"\\n=== 有效长度统计（非 pad 数量）===\")\n",
    "    print(f\"样本数: {tokens.size(0)}\")\n",
    "    print(f\"最短长度: {min_len_val}  (样本 index = {idx_min})\")\n",
    "    print(f\"最长长度: {max_len_val}  (样本 index = {idx_max})\")\n",
    "\n",
    "    # 3. 统计 eos 的情况\n",
    "    num_no_eos = 0\n",
    "    num_eos_at_last_nonpad = 0\n",
    "    num_eos_before_last_nonpad = 0\n",
    "\n",
    "    eos_positions = []  # 保存每个样本的 eos 位置（如果存在）\n",
    "\n",
    "    for i in range(tokens.size(0)):\n",
    "        row = tokens[i]\n",
    "        # 找到所有 eos 的位置（通常应该只有一个）\n",
    "        eos_idx = (row == eos_id).nonzero(as_tuple=True)[0]\n",
    "        if eos_idx.numel() == 0:\n",
    "            num_no_eos += 1\n",
    "            eos_positions.append(None)\n",
    "            continue\n",
    "\n",
    "        # 如果有多个 eos，就取最后一个（通常不会这样）\n",
    "        eos_pos = int(eos_idx[-1])\n",
    "        eos_positions.append(eos_pos)\n",
    "\n",
    "        last_nonpad_pos = int(lengths[i].item() - 1)  # 最后一个非 pad 的下标\n",
    "\n",
    "        if eos_pos == last_nonpad_pos:\n",
    "            num_eos_at_last_nonpad += 1\n",
    "        elif eos_pos < last_nonpad_pos:\n",
    "            num_eos_before_last_nonpad += 1\n",
    "        else:\n",
    "            # eos 在 pad 区域之后（理论上不太应该发生），也可以打印出来看\n",
    "            pass\n",
    "\n",
    "    print(\"\\n=== <eos> 位置统计 ===\")\n",
    "    print(f\"没有 eos 的样本数: {num_no_eos}\")\n",
    "    print(f\"eos 正好在最后一个非 pad 位置的样本数: {num_eos_at_last_nonpad}\")\n",
    "    print(f\"eos 出现在最后一个非 pad 之前的样本数: {num_eos_before_last_nonpad}\")\n",
    "\n",
    "    # 4. 打印几个指定样本，查看 eos 前后 token id / 文本\n",
    "    def show_sample(idx: int):\n",
    "        if idx < 0 or idx >= tokens.size(0):\n",
    "            print(f\"\\n样本 index {idx} 越界，跳过\")\n",
    "            return\n",
    "\n",
    "        row = tokens[idx]\n",
    "        length = int(lengths[idx].item())\n",
    "        eos_pos = eos_positions[idx]\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"样本 index = {idx}\")\n",
    "        print(f\"有效长度（非 pad） = {length}\")\n",
    "        print(f\"eos 位置 = {eos_pos}\")\n",
    "\n",
    "        # 打印整行 token id 前若干个（防止太长）\n",
    "        print(\"前 30 个 token id:\")\n",
    "        print(row[:30].tolist())\n",
    "\n",
    "        if eos_pos is not None:\n",
    "            start = max(0, eos_pos - WINDOW)\n",
    "            end = min(row.size(0), eos_pos + WINDOW + 1)\n",
    "            print(f\"\\n[eos] 附近 token id （从 {start} 到 {end - 1}）:\")\n",
    "            print(row[start:end].tolist())\n",
    "\n",
    "            # 如果 tokenizer 支持 decode，可以尝试解成文本看看\n",
    "            if hasattr(corpus.tokenizer, \"decode\"):\n",
    "                try:\n",
    "                    # 这里只看不含 pad 的一段\n",
    "                    ids_segment = row[start:end].tolist()\n",
    "                    text = corpus.tokenizer.decode(ids_segment)\n",
    "                    print(\"\\n[eos] 附近 decode 文本:\")\n",
    "                    print(text)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\ndecode 失败: {e}\")\n",
    "\n",
    "    # 把几个感兴趣的样本（手动指定 + 最短 + 最长）都看一下\n",
    "    all_to_check = set(CHECK_INDICES + [idx_min, idx_max])\n",
    "    for idx in sorted(all_to_check):\n",
    "        show_sample(idx)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
