{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0f912a",
   "metadata": {},
   "source": [
    "## 1. check corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c495fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ================= é…ç½®åŒºåŸŸ =================\n",
    "# è¯·ç¡®è®¤è¿™æ˜¯ä½ ç”¨æ¥æ„å»º corpus çš„åŸå§‹ CSV è·¯å¾„\n",
    "csv_path = \"/Users/kiancai/STA24/CWD/STAi/MiCoGPT/data/try2_withCC/abundance_B_13901.csv\"\n",
    "# ===========================================\n",
    "\n",
    "# --- 1. åŸºç¡€æ¦‚è§ˆä¸æ ·æœ¬æ£€æŸ¥ (ä»…æŸ¥çœ‹æœ€çŸ­å’Œæœ€é•¿) ---\n",
    "pad_id = corpus.tokenizer.pad_token_id\n",
    "real_lengths = (corpus.tokens != pad_id).sum(dim=1)\n",
    "\n",
    "targets = [\n",
    "    (\"ğŸŸ¢ æœ€çŸ­ (Shortest)\", torch.argmin(real_lengths).item()),\n",
    "    (\"ğŸ”´ æœ€é•¿ (Longest) \", torch.argmax(real_lengths).item())\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“Š [æ¦‚è§ˆ] æ ·æœ¬æ€»æ•°: {len(corpus)} | æœ€çŸ­é•¿åº¦: {real_lengths.min()} | æœ€é•¿é•¿åº¦: {real_lengths.max()}\")\n",
    "\n",
    "for title, idx in targets:\n",
    "    ids = corpus[idx]['input_ids']\n",
    "    valid_len = real_lengths[idx].item()\n",
    "    \n",
    "    print(f\"\\n{'='*40}\\nğŸ§ {title} | Index: {idx} | é•¿åº¦: {valid_len}/{len(ids)}\\n{'-'*40}\")\n",
    "    print(f\"{'Pos':<5} {'ID':<6} {'Token'}\")\n",
    "    \n",
    "    # æ™ºèƒ½åˆ‡ç‰‡ï¼šå¼€å¤´5ä¸ª ... ä¸­é—´(EOSé™„è¿‘) ... ç»“å°¾\n",
    "    indices = sorted(list(set(\n",
    "        list(range(0, 5)) + \n",
    "        list(range(valid_len - 2, valid_len + 3)) + \n",
    "        list(range(len(ids) - 2, len(ids)))\n",
    "    )))\n",
    "\n",
    "    last_i = -1\n",
    "    for i in indices:\n",
    "        if i >= len(ids): continue\n",
    "        if last_i != -1 and i > last_i + 1:\n",
    "            print(f\"{'...':<5} {'...':<6} ...\")\n",
    "        \n",
    "        token = corpus.tokenizer.convert_ids_to_tokens(ids[i].item())\n",
    "        print(f\"{i:<5} {ids[i]:<6} {token}\")\n",
    "        last_i = i\n",
    "\n",
    "# --- 2. è¯è¡¨åˆ©ç”¨ç‡ä¸ä¸¢å¤±è¯Šæ–­ ---\n",
    "print(f\"\\n{'='*40}\\nğŸš€ è¯è¡¨åˆ©ç”¨ç‡è¯Šæ–­\\n{'='*40}\")\n",
    "\n",
    "# ç»Ÿè®¡å½“å‰ Corpus ä½¿ç”¨æƒ…å†µ\n",
    "unique_ids = set(torch.unique(corpus.tokens).tolist())\n",
    "all_vocab_ids = set(range(corpus.tokenizer.vocab_size))\n",
    "special_ids = set(corpus.tokenizer.convert_tokens_to_ids(['<pad>', '<bos>', '<eos>', '<mask>']))\n",
    "target_vocab_ids = all_vocab_ids - special_ids # ç†è®ºä¸Šåº”æœ‰çš„èŒå±ID\n",
    "\n",
    "used_vocab = len(unique_ids - special_ids)\n",
    "missing_ids = target_vocab_ids - unique_ids\n",
    "missing_genera = [corpus.tokenizer.convert_ids_to_tokens(i) for i in missing_ids]\n",
    "\n",
    "print(f\"ğŸ’¡ è¯è¡¨åˆ©ç”¨ç‡: {used_vocab}/{len(corpus.tokenizer)} ({used_vocab/len(corpus.tokenizer):.1%})\")\n",
    "print(f\"â“ æœªå‡ºç°çš„èŒå±æ•°é‡: {len(missing_genera)}\")\n",
    "\n",
    "# å¦‚æœæœ‰ä¸¢å¤±ï¼Œè¯»å– CSV è¿›è¡Œæ¯”å¯¹ (ä½¿ç”¨ä¿®æ­£åçš„æ­£åˆ™)\n",
    "if len(missing_genera) > 0:\n",
    "    print(f\"\\nğŸ“– æ­£åœ¨è¯»å– CSV æ ¸å¯¹: {csv_path}\")\n",
    "    \n",
    "    try:\n",
    "        df_raw = pd.read_csv(csv_path, index_col=0)\n",
    "        # å…³é”®ä¿®æ­£ï¼šä½¿ç”¨æ›´å®½å®¹çš„æ­£åˆ™æå–åˆ—åï¼Œç¡®ä¿èƒ½åŒ¹é…åˆ°å¸¦ç‰¹æ®Šå­—ç¬¦çš„èŒå\n",
    "        raw_genera = set(df_raw.columns.str.extract(r'(g__[^;]+)').squeeze().tolist())\n",
    "        \n",
    "        not_in_csv = [g for g in missing_genera if g not in raw_genera]\n",
    "        in_csv_but_zero = [g for g in missing_genera if g in raw_genera]\n",
    "        \n",
    "        if not_in_csv:\n",
    "            print(f\"\\nâŒ [ä¸¥é‡] {len(not_in_csv)} ä¸ªèŒåœ¨ CSV è¡¨å¤´ä¸­å®Œå…¨æ‰¾ä¸åˆ°:\")\n",
    "            print(f\"   (å¦‚æœæ•°é‡å¾ˆå¤šï¼Œè¯·æ£€æŸ¥ corpus.py ä¸­çš„æ­£åˆ™æ˜¯å¦å·²ä¿®æ”¹ä¸º r'(g__[^;]+)')\")\n",
    "            print(f\"   ç¤ºä¾‹: {not_in_csv[:5]} ...\")\n",
    "        \n",
    "        if in_csv_but_zero:\n",
    "            print(f\"\\nğŸ“‰ [æ­£å¸¸] {len(in_csv_but_zero)} ä¸ªèŒåœ¨ CSV ä¸­å­˜åœ¨ï¼Œä½†åœ¨è¯­æ–™åº“ä¸­æ¶ˆå¤±:\")\n",
    "            print(f\"   åŸå› : è¿™äº›èŒåœ¨æ‰€æœ‰æ ·æœ¬ä¸­çš„ä¸°åº¦å¯èƒ½å‡ä¸º 0ï¼Œæˆ–è€…è¢« Phylogeny æ ‘è¿‡æ»¤äº†ã€‚\")\n",
    "            print(f\"   ç¤ºä¾‹: {in_csv_but_zero[:5]} ...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è¯»å– CSV å¤±è´¥: {e}\")\n",
    "else:\n",
    "    print(\"âœ… å®Œç¾ï¼æ‰€æœ‰èŒå±éƒ½å·²åŒ…å« (åˆ©ç”¨ç‡ç¬¦åˆé¢„æœŸ)ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= é…ç½® =================\n",
    "save_path = project_root / \"data\" / \"try2_withCC/sample_length_stats_B.csv\"\n",
    "# =======================================\n",
    "\n",
    "print(\"ğŸ“Š å¼€å§‹ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„çœŸå®è¯è¡¨é•¿åº¦...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# æ£€æŸ¥ corpus æ˜¯å¦ä¿ç•™äº†åŸå§‹æ•°æ®\n",
    "if not hasattr(corpus, 'data') or corpus.data is None:\n",
    "    print(\"âŒ é”™è¯¯: corpus å¯¹è±¡ä¸­æ²¡æœ‰æ‰¾åˆ° .data å±æ€§ã€‚\")\n",
    "    print(\"å¯èƒ½æ˜¯å› ä¸ºæ„å»ºæ—¶ä¸ºäº†çœå†…å­˜æ‰§è¡Œäº† del self.dataã€‚\")\n",
    "    print(\"å¦‚æœè¿™æ ·ï¼Œä½ éœ€è¦é‡æ–°åŠ è½½ CSV åˆ°å˜é‡ (ä¾‹å¦‚ df_raw) å¹¶åœ¨æ­¤å¤„éå†å®ƒã€‚\")\n",
    "else:\n",
    "    # éå† corpus å†…éƒ¨å­˜å‚¨çš„åŸå§‹ dataframe\n",
    "    # corpus.data çš„ç´¢å¼•é€šå¸¸å°±æ˜¯ Sample ID\n",
    "    for sample_id in tqdm(corpus.data.index, desc=\"Analyzing\"):\n",
    "        \n",
    "        # 1. è·å–è¯¥æ ·æœ¬çš„ä¸€è¡Œæ•°æ®\n",
    "        row_data = corpus.data.loc[sample_id]\n",
    "        \n",
    "        # 2. è°ƒç”¨å†…éƒ¨å‡½æ•°è®¡ç®—é•¿åº¦\n",
    "        # _convert_to_token è¿”å›: (å¤„ç†åçš„tokenåˆ—è¡¨, åŸå§‹é•¿åº¦)\n",
    "        # æ³¨æ„ï¼šè¿™ä¸ª original_len æ˜¯åŒ…å« <bos> å’Œ <eos> çš„\n",
    "        _, original_len = corpus._convert_to_token(row_data)\n",
    "        \n",
    "        # 3. è®¡ç®—æˆªæ–­æƒ…å†µ\n",
    "        max_len = corpus.max_len\n",
    "        # å®é™…ä¿ç•™çš„æœ‰æ•ˆé•¿åº¦ (ä¸å« pad)\n",
    "        kept_len = min(original_len, max_len)\n",
    "        # è¢«ä¸¢å¼ƒçš„è¯æ•°é‡\n",
    "        truncated_count = max(0, original_len - max_len)\n",
    "        \n",
    "        results.append({\n",
    "            \"Sample_ID\": sample_id,\n",
    "            \"Original_Length\": original_len,   # åŸå§‹çœŸå®é•¿åº¦\n",
    "            \"Max_Limit\": max_len,              # è®¾å®šçš„ä¸Šé™ (512)\n",
    "            \"Truncated_Tokens\": truncated_count, # è¢«æˆªæ–­ä¸¢æ‰çš„è¯æ•°\n",
    "            \"Is_Truncated\": truncated_count > 0  # æ˜¯å¦å‘ç”Ÿäº†æˆªæ–­\n",
    "        })\n",
    "\n",
    "    # 4. ç”Ÿæˆ DataFrame å¹¶ä¿å­˜\n",
    "    df_stats = pd.DataFrame(results)\n",
    "    df_stats.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"\\nâœ… ç»Ÿè®¡å®Œæˆï¼CSV å·²ä¿å­˜åˆ°: {save_path}\")\n",
    "    \n",
    "    # 5. æ‰“å°ä¸€äº›æ‘˜è¦ä¿¡æ¯\n",
    "    trunc_num = df_stats['Is_Truncated'].sum()\n",
    "    print(f\"\\n--- æ‘˜è¦ ---\")\n",
    "    print(f\"æ€»æ ·æœ¬æ•°: {len(df_stats)}\")\n",
    "    print(f\"è¢«æˆªæ–­çš„æ ·æœ¬æ•°: {trunc_num} ({trunc_num/len(df_stats):.2%})\")\n",
    "    if trunc_num > 0:\n",
    "        max_loss = df_stats['Truncated_Tokens'].max()\n",
    "        print(f\"æœ€ä¸¥é‡çš„æˆªæ–­: ä¸¢æ‰äº† {max_loss} ä¸ªè¯\")\n",
    "        \n",
    "    # çœ‹çœ‹å‰å‡ è¡Œ\n",
    "    print(\"\\né¢„è§ˆç”Ÿæˆçš„è¡¨æ ¼:\")\n",
    "    print(df_stats.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
