{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81535bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import load\n",
    "from torch.utils.data import DataLoader, SequentialSampler, Subset\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from importlib.resources import files\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace\n",
    "\n",
    "from MiCoGPT.utils.pretrain import attach_gated_prior_to_gpt2\n",
    "\n",
    "\n",
    "def load_gated_model(model_dir: str, tokenizer, npz_path, g_min=0.0, init_w=0.1, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) config\n",
    "    config = GPT2Config.from_pretrained(model_dir)\n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    # 2) 先把结构替换成 gated（非常关键：否则 state_dict key 对不上）\n",
    "    attach_gated_prior_to_gpt2(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        npz_path=npz_path,\n",
    "        g_min=g_min,\n",
    "        init_w=init_w,\n",
    "    )\n",
    "\n",
    "    # 3) 加载权重（兼容 .bin / .safetensors）\n",
    "    bin_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "    st_path = os.path.join(model_dir, \"model.safetensors\")\n",
    "\n",
    "    if os.path.exists(st_path):\n",
    "        from safetensors.torch import load_file\n",
    "        state = load_file(st_path, device=\"cpu\")\n",
    "    else:\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, device\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loss_and_ppl_by_project(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_subset: Subset,\n",
    "    project_col=\"Project_ID\",\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "):\n",
    "    # 取出 base corpus + indices，对齐 metadata\n",
    "    base = test_subset.dataset\n",
    "    base_indices = np.array(test_subset.indices)\n",
    "    meta = base.metadata.iloc[base_indices]\n",
    "    proj_ids_all = meta[project_col].astype(str).to_numpy()\n",
    "\n",
    "    collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    dl = DataLoader(\n",
    "        test_subset,\n",
    "        sampler=SequentialSampler(test_subset),  # 保证顺序与 indices 对齐\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collator,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    total_loss_sum = 0.0\n",
    "    total_tok_sum = 0\n",
    "\n",
    "    # project 聚合\n",
    "    loss_sum = {}\n",
    "    tok_sum = {}\n",
    "    n_samples = {}\n",
    "\n",
    "    pos = 0\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for batch in tqdm(dl, desc=\"Eval on test\", total=len(dl)):\n",
    "        # batch: input_ids/attention_mask/labels (collator 会加 labels 并把 pad 置为 -100)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch[\"labels\"]\n",
    "        bsz = labels.size(0)\n",
    "\n",
    "        # 对齐 project_id（依赖 SequentialSampler 保证顺序一致）\n",
    "        proj_ids = proj_ids_all[pos:pos + bsz]\n",
    "        pos += bsz\n",
    "\n",
    "        out = model(input_ids=batch[\"input_ids\"], attention_mask=batch.get(\"attention_mask\", None))\n",
    "        logits = out.logits  # [B, T, V]\n",
    "\n",
    "        # causal LM shift\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        V = shift_logits.size(-1)\n",
    "        loss_flat = F.cross_entropy(\n",
    "            shift_logits.view(-1, V),\n",
    "            shift_labels.view(-1),\n",
    "            ignore_index=-100,\n",
    "            reduction=\"none\",\n",
    "        ).view(bsz, -1)\n",
    "\n",
    "        mask = (shift_labels != -100)\n",
    "        loss_per_sample = (loss_flat * mask).sum(dim=1)         # [B] 每条样本 loss 总和\n",
    "        tok_per_sample = mask.sum(dim=1).to(torch.long)         # [B] 每条样本有效 token 数\n",
    "\n",
    "        # 总体\n",
    "        total_loss_sum += float(loss_per_sample.sum().item())\n",
    "        total_tok_sum += int(tok_per_sample.sum().item())\n",
    "\n",
    "        # 分 project\n",
    "        for pid, ls, nt in zip(proj_ids, loss_per_sample, tok_per_sample):\n",
    "            nt_i = int(nt.item())\n",
    "            if nt_i == 0:\n",
    "                continue\n",
    "            loss_sum[pid] = loss_sum.get(pid, 0.0) + float(ls.item())\n",
    "            tok_sum[pid] = tok_sum.get(pid, 0) + nt_i\n",
    "            n_samples[pid] = n_samples.get(pid, 0) + 1\n",
    "\n",
    "    overall_loss = total_loss_sum / max(total_tok_sum, 1)\n",
    "    overall_ppl = math.exp(overall_loss)\n",
    "\n",
    "    # 输出 project 级别 DataFrame\n",
    "    rows = []\n",
    "    for pid in sorted(loss_sum.keys()):\n",
    "        ploss = loss_sum[pid] / max(tok_sum[pid], 1)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"Project_ID\": pid,\n",
    "                \"n_samples\": n_samples.get(pid, 0),\n",
    "                \"n_tokens\": tok_sum.get(pid, 0),\n",
    "                \"loss\": ploss,\n",
    "                \"ppl\": math.exp(ploss),\n",
    "            }\n",
    "        )\n",
    "    df_by_project = pd.DataFrame(rows)\n",
    "\n",
    "    # 总体汇总（也做成一行）\n",
    "    df_overall = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"Project_ID\": \"ALL\",\n",
    "                \"n_samples\": len(test_subset),\n",
    "                \"n_tokens\": total_tok_sum,\n",
    "                \"loss\": overall_loss,\n",
    "                \"ppl\": overall_ppl,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return df_overall, df_by_project\n",
    "\n",
    "def load_base_model(model_dir: str, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab402714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/pretrain_ResMicroDB_90338_GATED_high_init_high_scale_random_group were not used when initializing GPT2LMHeadModel: ['transformer.wte.gate_logits', 'transformer.wte.prior_matrix', 'transformer.wte.base.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b605b41dffc3405b9950511be71eb702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval on test:   0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Project_ID  n_samples  n_tokens      loss        ppl\n",
      "0        ALL      13901    926639  4.096864  60.151336\n",
      "Saved: ../outputs/test_eval/pretrain_ResMicroDB_90338_GATED_high_init_high_scale_random_group_base/test_overall_loss_ppl.csv ../outputs/test_eval/pretrain_ResMicroDB_90338_GATED_high_init_high_scale_random_group_base/test_project_loss_ppl.csv\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    input=\"../data/try2_withCC/ResMicroDB_90338.pkl\",\n",
    "    models=\"../models/pretrain_ResMicroDB_90338_GATED_high_init_high_scale_random_group\",\n",
    "    test_dir=\"../outputs/test_eval/pretrain_ResMicroDB_90338_GATED_high_init_high_scale_random_group_base/\",\n",
    ")\n",
    "\n",
    "\n",
    "# 载入 corpus（你已经在训练脚本里这么做过）\n",
    "all_corpus = load(open(args.input, \"rb\"))\n",
    "tokenizer = all_corpus.tokenizer\n",
    "\n",
    "# test = Split_Group == \"B\"\n",
    "test_set = all_corpus.subset_by_metadata(lambda df: df[\"Split_Group\"] == \"B\")\n",
    "\n",
    "# 载入训练好的 gated 模型\n",
    "model_dir = args.models  # 你 trainer.save_model 保存的目录\n",
    "npz_path = files(\"MiCoGPT\")/\"resources\"/\"genus_embeddings_256.npz\"\n",
    "\n",
    "G_MIN = 0.00\n",
    "INIT_W = 0.50\n",
    "\n",
    "model, device = load_base_model(model_dir)\n",
    "\n",
    "# model, device = load_gated_model(\n",
    "#     model_dir=model_dir,\n",
    "#     tokenizer=tokenizer,\n",
    "#     npz_path=npz_path,\n",
    "#     g_min=G_MIN,\n",
    "#     init_w=INIT_W,\n",
    "# )\n",
    "\n",
    "# 评估并输出 csv\n",
    "df_all, df_proj = eval_loss_and_ppl_by_project(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_subset=test_set,\n",
    "    project_col=\"Project_ID\",\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "\n",
    "os.makedirs(args.test_dir, exist_ok=True)\n",
    "overall_path = os.path.join(args.test_dir, \"test_overall_loss_ppl.csv\")\n",
    "project_path = os.path.join(args.test_dir, \"test_project_loss_ppl.csv\")\n",
    "\n",
    "df_all.to_csv(overall_path, index=False)\n",
    "df_proj.to_csv(project_path, index=False)\n",
    "\n",
    "print(df_all)\n",
    "print(\"Saved:\", overall_path, project_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463682e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (caiqy_MiCoSeq_dev)",
   "language": "python",
   "name": "caiqy_micoseq_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
