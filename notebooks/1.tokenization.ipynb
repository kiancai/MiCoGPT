{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a87ac83",
   "metadata": {},
   "source": [
    "## 0. 导入依赖 & 基础配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a945b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pickle import dump, load   # 用 pickle 来保存 / 读取 tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "from typing import List\n",
    "\n",
    "# 物种组成表 CSV 路径，其第一列是带有前缀的物种名\n",
    "CSV_PATH = \"../data/try2_withCC/abundance_all_90338.csv\"\n",
    "TAXON_COLUMN = 0\n",
    "TOKENIZER_PKL_PATH = \"../MiCoGPT/resources/MiCoGPTokenizer.pkl\"\n",
    "OUTPUT_PHYLO_FILE = \"../MiCoGPT/resources/phylogeny.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f910198",
   "metadata": {},
   "source": [
    "## 0.1. 定义 “多前缀 → g__属名” 的转换函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d258f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_genus_like_microcorpus(raw_name: str) -> str:\n",
    "\n",
    "    if raw_name is None:\n",
    "        return None\n",
    "    \n",
    "    name = str(raw_name).strip()\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    name = name.replace(\"; \", \";\")             # 1) 去掉分号后的空格\n",
    "    name = re.sub(r\";s__.*\", \"\", name)         # 2) 去掉物种级：\";s__XXXXX\" 以及后面的内容\n",
    "    name = re.sub(r\"^k__\", \"sk__\", name)       # 3) 如果以 k__ 开头，换成 sk__\n",
    "    m = re.search(r\"(g__[^;]+)\", name)         # 4) 抽取 'g__XXXX'\n",
    "    \n",
    "    if m:\n",
    "        return m.group(1)   # 标准格式 g__Genus\n",
    "    #    - 如果本身就以 g__ 开头，直接用\n",
    "    if name.startswith(\"g__\"):\n",
    "        return name\n",
    "    #    - 否则退一步：取最后一级作为“属名”，同时打印 warning 方便你检查\n",
    "    #      比如 \"k__Bacteria; p__Firmicutes; ...; Bacteroides\"\n",
    "    parts = name.split(\";\")\n",
    "    fallback = parts[-1]\n",
    "    print(f\"[warning] 未找到 g__ 前缀，使用最后一级作为 token: {raw_name!r} -> {fallback!r}\")\n",
    "    return fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61918fa9",
   "metadata": {},
   "source": [
    "## A. 生成 MiCoGPTokenizer.pkl\n",
    "### A.1. 定义 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4652bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiCoGPTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    极简 tokenizer：\n",
    "    - 输入一个 token 列表（例如 ['<pad>', '<mask>', 'g__Bacteroides', ...]）\n",
    "    - 建立 token -> id 和 id -> token 映射\n",
    "    - 注册特殊 token: <pad>, <mask>, <bos>, <eos>\n",
    "    \"\"\"\n",
    "    def __init__(self, toks, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.toks = list(toks)\n",
    "        self.vocab = {v: i for i, v in enumerate(self.toks)}\n",
    "        self.ids_to_tokens = {i: v for i, v in enumerate(self.toks)}\n",
    "        \n",
    "        # 注册特殊 token\n",
    "        self.add_special_tokens({\n",
    "            'pad_token': '<pad>',\n",
    "            'mask_token': '<mask>',\n",
    "            'bos_token': '<bos>',\n",
    "            'eos_token': '<eos>',\n",
    "        })\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        return list(text)\n",
    "    \n",
    "    def _add_tokens(self, new_tokens: List[str], special_tokens: bool = False) -> int:\n",
    "        new_tokens = [tok for tok in new_tokens if tok not in self.vocab]\n",
    "        if not new_tokens:\n",
    "            return 0\n",
    "        \n",
    "        self.toks.extend(new_tokens)\n",
    "        self.vocab = {v: i for i, v in enumerate(self.toks)}\n",
    "        self.ids_to_tokens = {i: v for i, v in enumerate(self.toks)}\n",
    "        return len(new_tokens)\n",
    "    \n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.vocab[token]\n",
    "    \n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.ids_to_tokens[index]\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf39854f",
   "metadata": {},
   "source": [
    "### A.2. 读取 CSV，从第一列提取 genus 列表，去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f020a03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 维度: (1117, 90339)\n",
      "taxonomy 列非空行数: 1117\n",
      "经过处理后得到的 genus token 数量（含重复）: 1117\n",
      "去重后非特殊 token 数量: 1117\n",
      "前 10 个 genus token: ['g__Stenotrophomonas', 'g__Bacteriovorax', 'g__Idiomarina', 'g__Eubacterium', 'g__Methylobacillus', 'g__Larkinella', 'g__Fonticella', 'g__Klebsiella', 'g__Merdibacter', 'g__Fibrobacter']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"CSV 维度:\", df.shape)\n",
    "\n",
    "# 取出第一列(taxonomy)\n",
    "if isinstance(TAXON_COLUMN, int):\n",
    "    taxon_series = df.iloc[:, TAXON_COLUMN]\n",
    "else:\n",
    "    taxon_series = df[TAXON_COLUMN]\n",
    "\n",
    "print(\"taxonomy 列非空行数:\", taxon_series.notna().sum())\n",
    "\n",
    "# 对每一行做“多前缀 → g__属名” 处理\n",
    "genus_tokens = []\n",
    "for s in taxon_series:\n",
    "    g = extract_genus_like_microcorpus(s)\n",
    "    if g is not None:\n",
    "        genus_tokens.append(g)\n",
    "\n",
    "print(f\"经过处理后得到的 genus token 数量（含重复）: {len(genus_tokens)}\")\n",
    "\n",
    "# 去重\n",
    "seen = set()\n",
    "uniq_non_special_toks = []\n",
    "for t in genus_tokens:\n",
    "    if t not in seen:\n",
    "        uniq_non_special_toks.append(t)\n",
    "        seen.add(t)\n",
    "\n",
    "print(f\"去重后非特殊 token 数量: {len(uniq_non_special_toks)}\")\n",
    "print(\"前 10 个 genus token:\", uniq_non_special_toks[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587220a",
   "metadata": {},
   "source": [
    "### A.3. 构建 tokenizer 并保存为 tokenizer.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21887a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总 token 数量（包含 <pad>, <mask>，后续还会注册 <bos>, <eos>）: 1119\n",
      "MiCoGPTokenizer 已保存到: ../MiCoGPT/resources/MiCoGPTokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = ['<pad>', '<mask>']\n",
    "toks = SPECIAL_TOKENS + uniq_non_special_toks\n",
    "\n",
    "print(\"总 token 数量（包含 <pad>, <mask>，后续还会注册 <bos>, <eos>）:\", len(toks))\n",
    "\n",
    "tokenizer = MiCoGPTokenizer(toks)\n",
    "\n",
    "with open(TOKENIZER_PKL_PATH, \"wb\") as f:\n",
    "    dump(tokenizer, f)\n",
    "\n",
    "print(\"MiCoGPTokenizer 已保存到:\", TOKENIZER_PKL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89335b5",
   "metadata": {},
   "source": [
    "### A.4. 加载并检查 tokenizer.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef3fb928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的 vocab_size: 1121\n",
      "pad_token: <pad> id = 0\n",
      "bos_token: <bos> id = 1119\n",
      "eos_token: <eos> id = 1120\n",
      "\n",
      "前 10 个 token -> id 映射：\n",
      " 0: token='<pad>', id=0\n",
      " 1: token='<mask>', id=1\n",
      " 2: token='g__Stenotrophomonas', id=2\n",
      " 3: token='g__Bacteriovorax', id=3\n",
      " 4: token='g__Idiomarina', id=4\n",
      " 5: token='g__Eubacterium', id=5\n",
      " 6: token='g__Methylobacillus', id=6\n",
      " 7: token='g__Larkinella', id=7\n",
      " 8: token='g__Fonticella', id=8\n",
      " 9: token='g__Klebsiella', id=9\n"
     ]
    }
   ],
   "source": [
    "with open(TOKENIZER_PKL_PATH, \"rb\") as f:\n",
    "    tokenizer_loaded = load(f)\n",
    "\n",
    "print(\"载入后的 vocab_size:\", tokenizer_loaded.vocab_size)\n",
    "print(\"pad_token:\", tokenizer_loaded.pad_token, \"id =\", tokenizer_loaded.pad_token_id)\n",
    "print(\"bos_token:\", tokenizer_loaded.bos_token, \"id =\", tokenizer_loaded.bos_token_id)\n",
    "print(\"eos_token:\", tokenizer_loaded.eos_token, \"id =\", tokenizer_loaded.eos_token_id)\n",
    "\n",
    "print(\"\\n前 10 个 token -> id 映射：\")\n",
    "for i, (tok, idx) in enumerate(list(tokenizer_loaded.get_vocab().items())[:10]):\n",
    "    print(f\"{i:2d}: token={tok!r}, id={idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdcbfb",
   "metadata": {},
   "source": [
    "## B. 生成 phylogeny.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7df199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据: ../data/try2_withCC/abundance_all_90338.csv\n",
      "已生成: ../MiCoGPT/resources/phylogeny.csv\n",
      "                         mean       std\n",
      "#SampleID                              \n",
      "g__Stenotrophomonas  0.008533  0.070172\n",
      "g__Bacteriovorax     0.000018  0.000271\n",
      "g__Idiomarina        0.000002  0.000132\n",
      "g__Eubacterium       0.000002  0.000168\n",
      "g__Methylobacillus   0.000023  0.000711\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "print(f\"读取数据: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH, sep=',', index_col=0)\n",
    "\n",
    "# 处理索引名称 (Index)\n",
    "df.index = df.index.map(extract_genus_like_microcorpus)\n",
    "\n",
    "# 计算相对丰度 (Relative Abundance)\n",
    "df_rel = df.div(df.sum(axis=0), axis=1)\n",
    "\n",
    "# 计算 Mean 和 Std 并保存,构造结果 DataFrame\n",
    "stats_df = pd.DataFrame({\n",
    "    'mean': df_rel.mean(axis=1),\n",
    "    'std':  df_rel.std(axis=1).replace(0, 1e-9) # 防止方差为0\n",
    "})\n",
    "\n",
    "stats_df.index.name = '#SampleID' # 保持格式兼容\n",
    "\n",
    "stats_df.to_csv(OUTPUT_PHYLO_FILE)\n",
    "\n",
    "print(f\"已生成: {OUTPUT_PHYLO_FILE}\")\n",
    "print(stats_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
