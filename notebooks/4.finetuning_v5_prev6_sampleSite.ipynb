{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d36396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from configparser import ConfigParser\n",
    "from argparse import Namespace\n",
    "from pickle import load as pkl_load\n",
    "from importlib.resources import files\n",
    "from joblib import dump\n",
    "from torch.utils.data import Subset\n",
    "from transformers import Trainer,TrainingArguments,default_data_collator\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "from MiCoGPT.utils.finetune_v2 import prepare_labels_for_subset,get_raw_labels_from_subset,load_model_compat,SubsetWithLabels,FinetuneDataset\n",
    "from MiCoGPT.utils.finetune import split_train_val_by_project_stratified_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7cb285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(input='../data/try2_withCC/ResMicroDB_90338_new.pkl', model='../models/pretrain_ResMicroDB_90338_GATED_v6', output='../models/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v6_sampleSite', log='../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v6_sampleSite', val_split=0.2, label_col='Sample_Site', split_group='A', drop_na_label=True, g_min=0.0, batch_size=64, grad_accum=1, lr=1e-05, epochs=1000, patience=5, use_group_split=False, group_col='Project_ID')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = ConfigParser()\n",
    "cfg.read(\"../MiCoGPT/resources/config.ini\")\n",
    "\n",
    "input_corpus_path      = \"../data/try2_withCC/ResMicroDB_90338_new.pkl\"\n",
    "pretrained_model_path  = \"../models/pretrain_ResMicroDB_90338_GATED_v6\"\n",
    "output_model_dir       = \"../models/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v6_sampleSite\"\n",
    "log_dir                = \"../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v6_sampleSite\"\n",
    "val_split              = 0.2\n",
    "\n",
    "# ========= 新增：与你当前任务强相关的参数 =========\n",
    "label_col              = \"Sample_Site\"   # 你以前用的标签列\n",
    "subset_split_group     = \"A\"            # 只用 Split_Group == A\n",
    "drop_na_label          = True           # Is_Healthy is NA 的样本：直接丢弃（不训练也不预测）\n",
    "\n",
    "# gated-prior 相关（v6/v9 才用得到；普通 GPT2 无影响）\n",
    "g_min                  = 0.0            # 必须与 pretraining 时一致\n",
    "\n",
    "# 训练超参（你可以先用默认，后面再调）\n",
    "batch_size             = 64\n",
    "grad_accum             = 1\n",
    "lr                     = 1e-5\n",
    "epochs                 = 1000\n",
    "patience               = 5\n",
    "\n",
    "# （可选）按组划分，防止同一 project 泄漏\n",
    "use_group_split        = False\n",
    "group_col              = \"Project_ID\"\n",
    "\n",
    "args = Namespace(\n",
    "    input=input_corpus_path,\n",
    "    model=pretrained_model_path,\n",
    "    output=output_model_dir,\n",
    "    log=log_dir,\n",
    "    val_split=val_split,\n",
    "\n",
    "    label_col=label_col,\n",
    "    split_group=subset_split_group,\n",
    "    drop_na_label=drop_na_label,\n",
    "\n",
    "    g_min=g_min,\n",
    "\n",
    "    batch_size=batch_size,\n",
    "    grad_accum=grad_accum,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "\n",
    "    use_group_split=use_group_split,\n",
    "    group_col=group_col,\n",
    ")\n",
    "\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad952ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizer]\n",
      "  vocab_size: 1121\n",
      "  pad_token_id: 0\n",
      "  eos_token_id: 3\n",
      "Number of samples in all_corpus: 90338\n",
      "Number of samples in finetune_subset: 74557\n",
      "Split_Group\n",
      "A    74557\n",
      "B    13901\n",
      "C     1880\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 读取你保存的 MiCoGPTCorpus（all_corpus）\n",
    "with open(args.input, \"rb\") as f:\n",
    "    all_corpus = pkl_load(f)\n",
    "\n",
    "tokenizer = all_corpus.tokenizer\n",
    "\n",
    "# ====== 可选但强烈建议：确保 tokenizer 有 pad_token（GPT2 常见没有）======\n",
    "# 你的 corpus 通常已经有固定长度+attention_mask，但某些 collator/Trainer 仍可能关心 pad_token_id\n",
    "if getattr(tokenizer, \"pad_token_id\", None) is None:\n",
    "    # 常见做法：把 eos 当 pad（只要与你构建语料时的策略一致即可）\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"[Tokenizer]\")\n",
    "print(\"  vocab_size:\", getattr(tokenizer, \"vocab_size\", None))\n",
    "print(\"  pad_token_id:\", getattr(tokenizer, \"pad_token_id\", None))\n",
    "print(\"  eos_token_id:\", getattr(tokenizer, \"eos_token_id\", None))\n",
    "\n",
    "# 你想作为微调集合的样本（Split_Group == A 且 Is_Healthy 非空）\n",
    "finetune_subset = all_corpus.subset_by_metadata(\n",
    "    lambda df: (df[\"Split_Group\"] == args.split_group) & df[args.label_col].notna()\n",
    ")\n",
    "\n",
    "print(\"Number of samples in all_corpus:\", len(all_corpus))\n",
    "print(\"Number of samples in finetune_subset:\", len(finetune_subset))\n",
    "print(all_corpus.metadata[\"Split_Group\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526fc23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[labels] label_col = Sample_Site\n",
      "[labels] num_labels = 10\n",
      "[labels] label2id: {'BALF': 0, 'Bronchus': 1, 'Lung Tissue': 2, 'Nasal': 3, 'Nasopharynx': 4, 'Oropharynx': 5, 'Pharynx': 6, 'Sputum': 7, 'Throat': 8, 'Trachea': 9}\n",
      "[labels] label counts:\n",
      "Nasopharynx    18976\n",
      "Nasal          14183\n",
      "Sputum         12230\n",
      "Oropharynx      8407\n",
      "Trachea         6349\n",
      "BALF            5599\n",
      "Pharynx         5277\n",
      "Throat          2025\n",
      "Lung Tissue      782\n",
      "Bronchus         729\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tensor, all_labels, le, num_labels = prepare_labels_for_subset(\n",
    "    all_corpus=all_corpus,\n",
    "    subset=finetune_subset,\n",
    "    label_col=\"Sample_Site\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "labels_tensor[:10], num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1c7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[compat] Detected gated-prior checkpoint. gate_rank=1 (v6=1, v9=2)\n",
      "[compat] missing keys (first 20): ['score.weight']\n",
      "[compat] unexpected keys (first 20): ['lm_head.base.weight', 'lm_head.wte.gate_logits', 'lm_head.wte.prior_matrix', 'lm_head.wte.base.weight']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): GatedPriorEmbeddingCompat(\n",
       "      (base): Embedding(1121, 256)\n",
       "    )\n",
       "    (wpe): Embedding(512, 256)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x GPT2Block(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=256, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz_path = files(\"MiCoGPT\") / \"resources\" / \"genus_embeddings_256.npz\"  # 可留可删\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载器：\n",
    "#    - 如果 args.model 是普通 GPT2（或者你 base 的本地目录），直接加载 GPT2ForSequenceClassification\n",
    "#    - 如果 args.model 是 v6/v9 gated checkpoint，本地权重里会有 \"transformer.wte.base.weight\" 和 \"transformer.wte.gate_logits\"\n",
    "#      加载器会自动 patch wte，并识别 v6(1D gate) / v9(2D gate) 然后再把权重灌进去\n",
    "model = load_model_compat(\n",
    "    model_name_or_path=args.model,\n",
    "    num_labels=num_labels,\n",
    "    g_min=0.0,   # 这里务必与你预训练时 g_min 一致（你之前写的是 0.0 就保持 0.0）\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a13cbd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=True,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=1e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v6_sampleSite,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=5,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=eval_loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1000,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "output_dir=../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v6_sampleSite/finetune_checkpoints,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=64,\n",
       "per_device_train_batch_size=64,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v6_sampleSite/finetune_checkpoints,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=100,\n",
       "weight_decay=0.001,\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args_dict = {\n",
    "    # 学习率等超参从 config.ini 读（保留你的习惯）\n",
    "    \"learning_rate\": cfg.getfloat(\"finetune\", \"learning_rate\"),\n",
    "    \"warmup_steps\": cfg.getint(\"finetune\", \"warmup_steps\"),\n",
    "    \"weight_decay\": cfg.getfloat(\"finetune\", \"weight_decay\"),\n",
    "\n",
    "    # 训练/评估开关\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "\n",
    "    # 你的数据是定长 tensor（MiCoGPTCorpus 直接给 input_ids/attention_mask），不需要按长度分桶\n",
    "    \"group_by_length\": False,\n",
    "    \"disable_tqdm\": False,\n",
    "\n",
    "    # scheduler\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "\n",
    "    # batch & epoch\n",
    "    \"per_device_train_batch_size\": cfg.getint(\"finetune\", \"per_device_train_batch_size\"),\n",
    "    \"per_device_eval_batch_size\": cfg.getint(\"finetune\", \"per_device_train_batch_size\"),  # ✅ 建议显式设置，避免默认值不一致\n",
    "    \"num_train_epochs\": cfg.getint(\"finetune\", \"num_train_epochs\"),\n",
    "\n",
    "    # 保存与评估策略（保留你的 epoch 级别习惯）\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_steps\": cfg.getint(\"finetune\", \"logging_steps\"),\n",
    "\n",
    "    # 输出目录：注意 output_dir 是 checkpoint 的保存位置（你原来就这么写）\n",
    "    \"output_dir\": f\"{args.log}/finetune_checkpoints\",\n",
    "    \"logging_dir\": args.log,\n",
    "\n",
    "    # 选最优模型（用 eval_loss 最稳，不依赖自定义 metrics）\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(**training_args_dict)\n",
    "training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c9d052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[split] total_samples=74557, target_val~14911\n",
      "[split] eligible_projects=304, eligible_samples=74367\n",
      "[split] ineligible_projects=16, ineligible_samples=190\n",
      "[split] label_dist (overall):\n",
      "Sample_Site\n",
      "Nasopharynx    18976\n",
      "Nasal          14183\n",
      "Sputum         12230\n",
      "Oropharynx      8407\n",
      "Trachea         6349\n",
      "BALF            5599\n",
      "Pharynx         5277\n",
      "Throat          2025\n",
      "Lung Tissue      782\n",
      "Bronchus         729\n",
      "Name: count, dtype: int64\n",
      "[split] actual_val=14911 (target~14911), train=59646\n",
      "[split] label_dist (val):\n",
      "Sample_Site\n",
      "Nasopharynx    3809\n",
      "Nasal          2831\n",
      "Sputum         2444\n",
      "Oropharynx     1687\n",
      "Trachea        1264\n",
      "BALF           1116\n",
      "Pharynx        1062\n",
      "Throat          393\n",
      "Bronchus        157\n",
      "Lung Tissue     148\n",
      "Name: count, dtype: int64\n",
      "train_subset: 59646\n",
      "val_subset: 14911\n"
     ]
    }
   ],
   "source": [
    "train_subset, val_subset = split_train_val_by_project_stratified_with_labels(\n",
    "    finetune_subset,\n",
    "    label_col=\"Sample_Site\",\n",
    "    project_col=\"Project_ID\",\n",
    "    val_ratio=args.val_split,\n",
    "    min_project_samples=20,\n",
    "    min_val_per_project=2,\n",
    "    random_state=42,\n",
    "    label_balance_strength=0,  # 先用 1.0；想更强拉平就 2.0；不管标签就 0\n",
    ")\n",
    "\n",
    "print(\"train_subset:\", len(train_subset))\n",
    "print(\"val_subset:\", len(val_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ca7fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "train_idx range sample: [ 0  1  3  4  5  6  7  8  9 10]\n",
      "val_idx range sample: [ 2 18 24 26 27 46 50 53 57 74]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43804' max='932000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 43804/932000 3:58:03 < 80:27:19, 3.07 it/s, Epoch 47/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.147200</td>\n",
       "      <td>1.094643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.873600</td>\n",
       "      <td>0.785857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.658317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.578834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.540100</td>\n",
       "      <td>0.524830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.563200</td>\n",
       "      <td>0.482596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.451409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.425986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.423200</td>\n",
       "      <td>0.404584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.387363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.372131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.346453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.338593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.328827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.319358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.290700</td>\n",
       "      <td>0.314705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.306793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.301902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>0.298162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.292237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.288762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.181600</td>\n",
       "      <td>0.285519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.214700</td>\n",
       "      <td>0.281426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.279101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.201100</td>\n",
       "      <td>0.276050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>0.274221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.272931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>0.271581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.271127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.268387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.267562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.268764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.204200</td>\n",
       "      <td>0.266690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>0.268464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.267365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.266436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.266683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.267440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.268830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.268966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0.268439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>0.268537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.270591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.271664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.271574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.274600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and label encoder saved to: ../models/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v6_sampleSite\n",
      "Training logs saved to: ../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v6_sampleSite/finetune_log.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8803</th>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>46.98</td>\n",
       "      <td>43790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8804</th>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>46.99</td>\n",
       "      <td>43795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8805</th>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>47.00</td>\n",
       "      <td>43800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8806</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.00</td>\n",
       "      <td>43804</td>\n",
       "      <td>0.2746</td>\n",
       "      <td>22.195</td>\n",
       "      <td>671.819</td>\n",
       "      <td>10.498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8807</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.00</td>\n",
       "      <td>43804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14285.035</td>\n",
       "      <td>4175.419</td>\n",
       "      <td>65.243</td>\n",
       "      <td>5.444696e+16</td>\n",
       "      <td>0.30365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  learning_rate  epoch   step  eval_loss  eval_runtime  \\\n",
       "8803  0.0975        0.00001  46.98  43790        NaN           NaN   \n",
       "8804  0.1080        0.00001  46.99  43795        NaN           NaN   \n",
       "8805  0.0760        0.00001  47.00  43800        NaN           NaN   \n",
       "8806     NaN            NaN  47.00  43804     0.2746        22.195   \n",
       "8807     NaN            NaN  47.00  43804        NaN           NaN   \n",
       "\n",
       "      eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "8803                      NaN                    NaN            NaN   \n",
       "8804                      NaN                    NaN            NaN   \n",
       "8805                      NaN                    NaN            NaN   \n",
       "8806                  671.819                 10.498            NaN   \n",
       "8807                      NaN                    NaN      14285.035   \n",
       "\n",
       "      train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "8803                       NaN                     NaN           NaN   \n",
       "8804                       NaN                     NaN           NaN   \n",
       "8805                       NaN                     NaN           NaN   \n",
       "8806                       NaN                     NaN           NaN   \n",
       "8807                  4175.419                  65.243  5.444696e+16   \n",
       "\n",
       "      train_loss  \n",
       "8803         NaN  \n",
       "8804         NaN  \n",
       "8805         NaN  \n",
       "8806         NaN  \n",
       "8807     0.30365  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start training...\")\n",
    "\n",
    "# ========= 1) 解决：train_subset / val_subset 可能是“嵌套 Subset”，需要解析到最底层 base dataset =========\n",
    "def resolve_subset_to_base_and_indices(ds):\n",
    "    \"\"\"\n",
    "    把 Dataset/Subset 递归展开，返回：\n",
    "      base_dataset: 最底层数据集（通常应是 all_corpus）\n",
    "      base_indices: ds 在 base_dataset 上的绝对索引\n",
    "    \"\"\"\n",
    "    if not isinstance(ds, Subset):\n",
    "        return ds, np.arange(len(ds), dtype=int)\n",
    "\n",
    "    base, base_idx = resolve_subset_to_base_and_indices(ds.dataset)\n",
    "    cur_idx = np.asarray(ds.indices, dtype=int)\n",
    "    return base, base_idx[cur_idx]\n",
    "\n",
    "# 解析 train/val 到最底层 dataset + 绝对索引\n",
    "base_train, train_idx = resolve_subset_to_base_and_indices(train_subset)\n",
    "base_val,   val_idx   = resolve_subset_to_base_and_indices(val_subset)\n",
    "\n",
    "# 一般情况下两者都会指向同一个 base（通常是 all_corpus）\n",
    "assert base_train is base_val, \"train/val 的 base dataset 不一致，这通常不应该发生。\"\n",
    "base_corpus = base_train\n",
    "\n",
    "print(\"train_idx range sample:\", train_idx[:10])\n",
    "print(\"val_idx range sample:\", val_idx[:10])\n",
    "\n",
    "# ========= 2) 取原始标签（从 metadata 里拿），再用同一个 le 编码成 0..C-1 =========\n",
    "train_raw = base_corpus.metadata.iloc[train_idx][args.label_col]\n",
    "val_raw   = base_corpus.metadata.iloc[val_idx][args.label_col]\n",
    "\n",
    "# 你已经在 finetune_subset 里过滤过 notna，这里再兜底检查一次\n",
    "assert train_raw.notna().all(), \"train 中仍存在 NA 标签，请检查你的 subset 条件。\"\n",
    "assert val_raw.notna().all(),   \"val 中仍存在 NA 标签，请检查你的 subset 条件。\"\n",
    "\n",
    "# 用同一个 label encoder（le）做 transform，保证 train/val 编码一致\n",
    "train_labels = le.transform(train_raw.tolist())\n",
    "val_labels   = le.transform(val_raw.tolist())\n",
    "\n",
    "# 你之前的断言（现在标签不再是 -1，而是 0..C-1）\n",
    "assert (train_labels >= 0).all()\n",
    "assert (val_labels >= 0).all()\n",
    "\n",
    "# ========= 3) 构建 Trainer 可用的 Dataset（沿用你原来的 FinetuneDataset 风格） =========\n",
    "train_dataset = FinetuneDataset(base_corpus, train_idx, train_labels)\n",
    "val_dataset   = FinetuneDataset(base_corpus, val_idx,   val_labels)\n",
    "\n",
    "# ========= 4) callbacks + Trainer =========\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=callbacks,\n",
    "\n",
    "    # ⚠️ 关键：不要传 tokenizer（否则可能触发 tokenizer.save_pretrained -> NotImplementedError）\n",
    "    tokenizer=None,\n",
    "\n",
    "    # 你的样本本身就是 tensor + 定长，默认 collator 最稳\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ========= 5) 保存模型 =========\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "trainer.save_model(args.output)\n",
    "\n",
    "# 保存 label encoder（你原来怎么做就怎么做）\n",
    "dump(le, open(os.path.join(args.output, \"label_encoder.pkl\"), \"wb\"))\n",
    "print(f\"Model and label encoder saved to: {args.output}\")\n",
    "\n",
    "# ========= 6) 保存日志 =========\n",
    "logs = trainer.state.log_history\n",
    "logs_df = pd.DataFrame(logs)\n",
    "\n",
    "os.makedirs(args.log, exist_ok=True)\n",
    "log_path = os.path.join(args.log, \"finetune_log.csv\")\n",
    "logs_df.to_csv(log_path, index=False)\n",
    "\n",
    "print(f\"Training logs saved to: {log_path}\")\n",
    "logs_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c32b2-f704-44c0-b77e-c801397d6831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (caiqy_MiCoSeq_dev)",
   "language": "python",
   "name": "caiqy_micoseq_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
