{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5325b9",
   "metadata": {},
   "source": [
    "## 0. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca57f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))  # 从 notebooks/ 回到外面的 MiCoGPT 根目录\n",
    "from MiCoGPT.utils.corpus import (\n",
    "    SequenceClassificationDataset,\n",
    ")\n",
    "\n",
    "from pickle import load, dump\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import (\n",
    "    GPT2ForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "from configparser import ConfigParser\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e568c13",
   "metadata": {},
   "source": [
    "## 1. 加载 / 设置配置（cfg）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336c6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[finetune] section:\n",
      "learning_rate = 1e-3\n",
      "warmup_steps = 100\n",
      "weight_decay = 0.001\n",
      "per_device_train_batch_size = 64\n",
      "num_train_epochs = 1000\n",
      "logging_steps = 5\n"
     ]
    }
   ],
   "source": [
    "cfg_path = \"../MiCoGPT/resources/mgm_config.ini\"\n",
    "\n",
    "cfg = ConfigParser()\n",
    "cfg.read(cfg_path)\n",
    "\n",
    "print(\"[finetune] section:\")\n",
    "for k, v in cfg[\"finetune\"].items():\n",
    "    print(f\"{k} = {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d804d",
   "metadata": {},
   "source": [
    "## 2. 设置参数 args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ae1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(input='../data/try2_withCC/abundance_A_subgroup_55575.pkl', labels='../data/try2_withCC/flag_A_subgroup_55575.csv', model='../models/pretrain_A_74557', output='../models/finetuned_model_A_subset_55575', log='../logs', val_split=0.2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_corpus_path = \"../data/try2_withCC/ResMicroDB_90338.pkl\"\n",
    "# labels_csv_path   = \"../data/try2_withCC/flag_A_subgroup_55575.csv\"\n",
    "pretrained_model_path = \"../models/pretrain_ResMicroDB_90338\"\n",
    "output_model_dir  = \"../models/finetuned_model_ResMicroDB_90338\"\n",
    "log_dir           = \"../logs/finetuned_ResMicroDB_90338\"\n",
    "val_split         = 0.2         # 验证集比例\n",
    "\n",
    "args = Namespace(\n",
    "    input=input_corpus_path,\n",
    "    # labels=labels_csv_path,\n",
    "    model=pretrained_model_path,\n",
    "    output=output_model_dir,\n",
    "    log=log_dir,\n",
    "    val_split=val_split,\n",
    ")\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65da06b",
   "metadata": {},
   "source": [
    "## 3. 加载语料与标签，检查样本 ID 一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e92cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 55575\n",
      "labels size: 55575\n",
      "First few labels:\n",
      "            Is_Healthy\n",
      "CRR768228       False\n",
      "CRR768229       False\n",
      "CRR768230       False\n",
      "CRR768231       False\n",
      "CRR768232       False\n"
     ]
    }
   ],
   "source": [
    "corpus = load(open(args.input, \"rb\"))\n",
    "tokenizer = corpus.tokenizer\n",
    "\n",
    "labels = pd.read_csv(args.labels, index_col=0)\n",
    "\n",
    "# 样本 ID 对齐检查\n",
    "if set(corpus.data.index) != set(labels.index):\n",
    "    print(\n",
    "        \"Warning: the sample IDs in the abundance table and the metadata table are not the same.\"\n",
    "        \"The samples in the metadata table but not in the abundance table will be removed.\"\n",
    "        \"This may happened because some samples were removed or had all zero counts during the preprocessing of the abundance table.\"\n",
    "    )\n",
    "\n",
    "# 只保留与 corpus 一致的样本顺序\n",
    "labels = labels.loc[corpus.data.index]\n",
    "\n",
    "print(\"corpus size:\", len(corpus))\n",
    "print(\"labels size:\", len(labels))\n",
    "print(\"First few labels:\\n\", labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f4302",
   "metadata": {},
   "source": [
    "## 4. 标签编码（OneHotEncoder → class index）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5edfea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels: 2\n"
     ]
    }
   ],
   "source": [
    "# label encoding（完全照原始代码）\n",
    "le = OneHotEncoder()\n",
    "labels_arr = le.fit_transform(labels.values.reshape(-1, 1)).toarray()\n",
    "labels_tensor = torch.tensor(labels_arr.argmax(axis=1))\n",
    "\n",
    "num_labels = len(le.categories_[0])\n",
    "print(\"num_labels:\", num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4392e",
   "metadata": {},
   "source": [
    "## 5. 打包成 Dataset（SequenceClassificationDataset）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25ad7056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiancai/STA24/CWD/STAi/MiCoGPT/MiCoGPT/utils/corpus.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = self.tokens[index].clone()\n",
      "/Users/kiancai/STA24/CWD/STAi/MiCoGPT/MiCoGPT/utils/corpus.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  def __getitem__(self, idx):\n",
      "/Users/kiancai/STA24/CWD/STAi/MiCoGPT/MiCoGPT/utils/corpus.py:160: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\n",
      "/Users/kiancai/STA24/CWD/STAi/MiCoGPT/MiCoGPT/utils/corpus.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"input_ids\": torch.tensor(self.seq[idx]),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55575,\n",
       " {'input_ids': tensor([1119,  710,  397,  474,  675,  368,  191,  701,   20,  632,  138,  357,\n",
       "           343,  490,  466,  307,  102,  403,   61,  791,  625,  672,  127,  156,\n",
       "           267,  168,  203,  746,   60,  450,  749,   10,  380,  384,  178,  269,\n",
       "           591,  524,  538,  220,  500,  136,  719,  768,  786,   48,  405,  145,\n",
       "           301,  296,   63,  562,  270,  531,  104,  426,  717,  642,  130,  311,\n",
       "            56,  299,  517,  142,  568,  595,  144,  371,  681,  155,  381,  383,\n",
       "            41,  328,  437,  309,  321,  794,  670,  506,  648,  360,  470,  736,\n",
       "           443,  495,  565,  364,  448,  576,  124,  604,   79,  223,  482,  282,\n",
       "           365,  205,  339,  480,  527,    9,  501,  588,  489,    2,  430,  782,\n",
       "           358,  756,  486,  751,  572,  345,  342,  278,  526,  505,  333,   90,\n",
       "           460,  599,   36,  372,  512,  396,  618,  305,  359,   84,  481, 1120,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]),\n",
       "  'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'labels': tensor(0)})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对应原始代码中的 SequenceClassificationDataset 封装\n",
    "dataset = SequenceClassificationDataset(\n",
    "    corpus[:][\"input_ids\"],\n",
    "    corpus[:][\"attention_mask\"],\n",
    "    labels_tensor,\n",
    ")\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d3736",
   "metadata": {},
   "source": [
    "## 6. 加载预训练模型（GPT2ForSequenceClassification）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72cd0ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ../models/pretrain_A_74557 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(1121, 256)\n",
       "    (wpe): Embedding(512, 256)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x GPT2Block(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=256, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model config\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    args.model,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be2f31",
   "metadata": {},
   "source": [
    "## 7. 构造 TrainingArguments（从 cfg 读取超参）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89db3fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=True,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=True,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=0.001,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=../logs,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=5,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1000,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "output_dir=../logs/finetune_checkpoints,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=64,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=../logs/finetune_checkpoints,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=100,\n",
       "weight_decay=0.001,\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args_dict = {\n",
    "    \"learning_rate\": cfg.getfloat(\"finetune\", \"learning_rate\"),\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": False,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"warmup_steps\": cfg.getint(\"finetune\", \"warmup_steps\"),\n",
    "    \"weight_decay\": cfg.getfloat(\"finetune\", \"weight_decay\"),\n",
    "    \"per_device_train_batch_size\": cfg.getint(\"finetune\", \"per_device_train_batch_size\"),\n",
    "    \"num_train_epochs\": cfg.getint(\"finetune\", \"num_train_epochs\"),\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_steps\": cfg.getint(\"finetune\", \"logging_steps\"),\n",
    "    \"output_dir\": f\"{args.log}/finetune_checkpoints\",\n",
    "    \"logging_dir\": args.log,\n",
    "    \"load_best_model_at_end\": True,\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(**training_args_dict)\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3640a51",
   "metadata": {},
   "source": [
    "## 8. 划分训练 / 验证集 + 构建 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d969f24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "train_size = 44460, val_size = 11115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x172d9d630>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start training...\")\n",
    "model = model.train()\n",
    "\n",
    "split = args.val_split\n",
    "\n",
    "train_size = int(len(corpus) * (1 - split))\n",
    "val_size = len(corpus) - train_size  # 保证两者之和等于总长度\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"train_size = {train_size}, val_size = {val_size}\")\n",
    "\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8267e",
   "metadata": {},
   "source": [
    "## 9. 开始训练 + 保存模型和 label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f50c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "trainer.save_model(args.output)\n",
    "\n",
    "# 保存 label encoder\n",
    "dump(le, open(os.path.join(args.output, \"label_encoder.pkl\"), \"wb\"))\n",
    "print(f\"Model and label encoder saved to: {args.output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202002af",
   "metadata": {},
   "source": [
    "## 10. 保存训练日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = trainer.state.log_history\n",
    "logs_df = pd.DataFrame(logs)\n",
    "\n",
    "os.makedirs(args.log, exist_ok=True)\n",
    "log_path = os.path.join(args.log, \"finetune_log.csv\")\n",
    "logs_df.to_csv(log_path, index=False)\n",
    "\n",
    "print(f\"Training logs saved to: {log_path}\")\n",
    "logs_df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
