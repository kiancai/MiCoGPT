{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d36396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from configparser import ConfigParser\n",
    "from argparse import Namespace\n",
    "from pickle import load as pkl_load\n",
    "from importlib.resources import files\n",
    "from joblib import dump\n",
    "from torch.utils.data import Subset\n",
    "from transformers import Trainer,TrainingArguments,default_data_collator\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "from MiCoGPT.utils.finetune_v2 import prepare_labels_for_subset,get_raw_labels_from_subset,load_model_compat,SubsetWithLabels,FinetuneDataset\n",
    "from MiCoGPT.utils.finetune import split_train_val_by_project_stratified_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cb285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(input='../data/try2_withCC/ResMicroDB_90338.pkl', model='../models/pretrain_ResMicroDB_90338_GATED_v9_5000', output='../models/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v9_5000', log='../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v9_5000', val_split=0.2, label_col='Is_Healthy', split_group='A', drop_na_label=True, g_min=0.0, batch_size=64, grad_accum=1, lr=1e-05, epochs=1000, patience=5, use_group_split=False, group_col='Project_ID')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = ConfigParser()\n",
    "cfg.read(\"../MiCoGPT/resources/config.ini\")\n",
    "\n",
    "input_corpus_path      = \"../data/try2_withCC/ResMicroDB_90338_with_meta_v2.pkl\"\n",
    "pretrained_model_path  = \"../models/pretrain_ResMicroDB_90338_GATED_sampleSite_v9_5000\"\n",
    "output_model_dir       = \"../models/finetuned_v5_pretrain_ResMicroDB_90338_GATED_PREsampleSite_v9_5000\"\n",
    "log_dir                = \"../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_PREsampleSite_v9_5000\"\n",
    "val_split              = 0.2\n",
    "\n",
    "# ========= 新增：与你当前任务强相关的参数 =========\n",
    "label_col              = \"Is_Healthy\"   # 你以前用的标签列\n",
    "subset_split_group     = \"A\"            # 只用 Split_Group == A\n",
    "drop_na_label          = True           # Is_Healthy is NA 的样本：直接丢弃（不训练也不预测）\n",
    "\n",
    "# gated-prior 相关（v6/v9 才用得到；普通 GPT2 无影响）\n",
    "g_min                  = 0.0            # 必须与 pretraining 时一致\n",
    "\n",
    "# 训练超参（你可以先用默认，后面再调）\n",
    "batch_size             = 64\n",
    "grad_accum             = 1\n",
    "lr                     = 1e-5\n",
    "epochs                 = 1000\n",
    "patience               = 5\n",
    "\n",
    "# （可选）按组划分，防止同一 project 泄漏\n",
    "use_group_split        = False\n",
    "group_col              = \"Project_ID\"\n",
    "\n",
    "args = Namespace(\n",
    "    input=input_corpus_path,\n",
    "    model=pretrained_model_path,\n",
    "    output=output_model_dir,\n",
    "    log=log_dir,\n",
    "    val_split=val_split,\n",
    "\n",
    "    label_col=label_col,\n",
    "    split_group=subset_split_group,\n",
    "    drop_na_label=drop_na_label,\n",
    "\n",
    "    g_min=g_min,\n",
    "\n",
    "    batch_size=batch_size,\n",
    "    grad_accum=grad_accum,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "\n",
    "    use_group_split=use_group_split,\n",
    "    group_col=group_col,\n",
    ")\n",
    "\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad952ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizer]\n",
      "  vocab_size: 1121\n",
      "  pad_token_id: 0\n",
      "  eos_token_id: 3\n",
      "Number of samples in all_corpus: 90338\n",
      "Number of samples in finetune_subset: 55575\n",
      "Split_Group\n",
      "A    74557\n",
      "B    13901\n",
      "C     1880\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 读取你保存的 MiCoGPTCorpus（all_corpus）\n",
    "with open(args.input, \"rb\") as f:\n",
    "    all_corpus = pkl_load(f)\n",
    "\n",
    "tokenizer = all_corpus.tokenizer\n",
    "\n",
    "# ====== 可选但强烈建议：确保 tokenizer 有 pad_token（GPT2 常见没有）======\n",
    "# 你的 corpus 通常已经有固定长度+attention_mask，但某些 collator/Trainer 仍可能关心 pad_token_id\n",
    "if getattr(tokenizer, \"pad_token_id\", None) is None:\n",
    "    # 常见做法：把 eos 当 pad（只要与你构建语料时的策略一致即可）\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"[Tokenizer]\")\n",
    "print(\"  vocab_size:\", getattr(tokenizer, \"vocab_size\", None))\n",
    "print(\"  pad_token_id:\", getattr(tokenizer, \"pad_token_id\", None))\n",
    "print(\"  eos_token_id:\", getattr(tokenizer, \"eos_token_id\", None))\n",
    "\n",
    "# 你想作为微调集合的样本（Split_Group == A 且 Is_Healthy 非空）\n",
    "finetune_subset = all_corpus.subset_by_metadata(\n",
    "    lambda df: (df[\"Split_Group\"] == args.split_group) & df[args.label_col].notna()\n",
    ")\n",
    "\n",
    "print(\"Number of samples in all_corpus:\", len(all_corpus))\n",
    "print(\"Number of samples in finetune_subset:\", len(finetune_subset))\n",
    "print(all_corpus.metadata[\"Split_Group\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526fc23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[labels] label_col = Is_Healthy\n",
      "[labels] num_labels = 2\n",
      "[labels] label2id: {'False': 0, 'True': 1}\n",
      "[labels] label counts:\n",
      "False    31073\n",
      "True     24502\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tensor, all_labels, le, num_labels = prepare_labels_for_subset(\n",
    "    all_corpus=all_corpus,\n",
    "    subset=finetune_subset,\n",
    "    label_col=\"Is_Healthy\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "labels_tensor[:10], num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1c7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[compat] Detected gated-prior checkpoint. gate_rank=2 (v6=1, v9=2)\n",
      "[compat] missing keys (first 20): ['score.weight']\n",
      "[compat] unexpected keys (first 20): ['lm_head.base.weight', 'lm_head.wte.gate_logits', 'lm_head.wte.prior_matrix', 'lm_head.wte.base.weight']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): GatedPriorEmbeddingCompat(\n",
       "      (base): Embedding(1121, 256)\n",
       "    )\n",
       "    (wpe): Embedding(512, 256)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x GPT2Block(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=256, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz_path = files(\"MiCoGPT\") / \"resources\" / \"genus_embeddings_256.npz\"  # 可留可删\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载器：\n",
    "#    - 如果 args.model 是普通 GPT2（或者你 base 的本地目录），直接加载 GPT2ForSequenceClassification\n",
    "#    - 如果 args.model 是 v6/v9 gated checkpoint，本地权重里会有 \"transformer.wte.base.weight\" 和 \"transformer.wte.gate_logits\"\n",
    "#      加载器会自动 patch wte，并识别 v6(1D gate) / v9(2D gate) 然后再把权重灌进去\n",
    "model = load_model_compat(\n",
    "    model_name_or_path=args.model,\n",
    "    num_labels=num_labels,\n",
    "    g_min=0.0,   # 这里务必与你预训练时 g_min 一致（你之前写的是 0.0 就保持 0.0）\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13cbd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=True,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=1e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v9_5000,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=5,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=eval_loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1000,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "output_dir=../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v9_5000/finetune_checkpoints,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=64,\n",
       "per_device_train_batch_size=64,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v9_5000/finetune_checkpoints,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=100,\n",
       "weight_decay=0.001,\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args_dict = {\n",
    "    # 学习率等超参从 config.ini 读（保留你的习惯）\n",
    "    \"learning_rate\": cfg.getfloat(\"finetune\", \"learning_rate\"),\n",
    "    \"warmup_steps\": cfg.getint(\"finetune\", \"warmup_steps\"),\n",
    "    \"weight_decay\": cfg.getfloat(\"finetune\", \"weight_decay\"),\n",
    "\n",
    "    # 训练/评估开关\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "\n",
    "    # 你的数据是定长 tensor（MiCoGPTCorpus 直接给 input_ids/attention_mask），不需要按长度分桶\n",
    "    \"group_by_length\": False,\n",
    "    \"disable_tqdm\": False,\n",
    "\n",
    "    # scheduler\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "\n",
    "    # batch & epoch\n",
    "    \"per_device_train_batch_size\": cfg.getint(\"finetune\", \"per_device_train_batch_size\"),\n",
    "    \"per_device_eval_batch_size\": cfg.getint(\"finetune\", \"per_device_train_batch_size\"),  # ✅ 建议显式设置，避免默认值不一致\n",
    "    \"num_train_epochs\": cfg.getint(\"finetune\", \"num_train_epochs\"),\n",
    "\n",
    "    # 保存与评估策略（保留你的 epoch 级别习惯）\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_steps\": cfg.getint(\"finetune\", \"logging_steps\"),\n",
    "\n",
    "    # 输出目录：注意 output_dir 是 checkpoint 的保存位置（你原来就这么写）\n",
    "    \"output_dir\": f\"{args.log}/finetune_checkpoints\",\n",
    "    \"logging_dir\": args.log,\n",
    "\n",
    "    # 选最优模型（用 eval_loss 最稳，不依赖自定义 metrics）\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(**training_args_dict)\n",
    "training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c9d052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[split] total_samples=55575, target_val~11115\n",
      "[split] eligible_projects=251, eligible_samples=55398\n",
      "[split] ineligible_projects=16, ineligible_samples=177\n",
      "[split] label_dist (overall):\n",
      "Is_Healthy\n",
      "False    31073\n",
      "True     24502\n",
      "Name: count, dtype: int64\n",
      "[split] actual_val=11115 (target~11115), train=44460\n",
      "[split] label_dist (val):\n",
      "Is_Healthy\n",
      "False    6191\n",
      "True     4924\n",
      "Name: count, dtype: int64\n",
      "train_subset: 44460\n",
      "val_subset: 11115\n"
     ]
    }
   ],
   "source": [
    "train_subset, val_subset = split_train_val_by_project_stratified_with_labels(\n",
    "    finetune_subset,\n",
    "    label_col=\"Is_Healthy\",\n",
    "    project_col=\"Project_ID\",\n",
    "    val_ratio=args.val_split,\n",
    "    min_project_samples=20,\n",
    "    min_val_per_project=2,\n",
    "    random_state=42,\n",
    "    label_balance_strength=0,  # 先用 1.0；想更强拉平就 2.0；不管标签就 0\n",
    ")\n",
    "\n",
    "print(\"train_subset:\", len(train_subset))\n",
    "print(\"val_subset:\", len(val_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca7fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "train_idx range sample: [ 1  2  3  4  5  6  9 10 11 12]\n",
      "val_idx range sample: [ 0  7  8 21 26 29 32 35 39 40]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22935' max='695000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22935/695000 2:10:54 < 63:56:07, 2.92 it/s, Epoch 33/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.350799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.276567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.239470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.214973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.230700</td>\n",
       "      <td>0.199957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.199100</td>\n",
       "      <td>0.188737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.181240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.173834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.165716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.164690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.157373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.154122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.110700</td>\n",
       "      <td>0.151574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.108600</td>\n",
       "      <td>0.145835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.148114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.098500</td>\n",
       "      <td>0.146022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.143312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.144836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.145527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.119600</td>\n",
       "      <td>0.143841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.144554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.146058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.142848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.151032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.146145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.153569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.152163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.158295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.157811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.158459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.163837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.168756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.174353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and label encoder saved to: ../models/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v9_5000\n",
      "Training logs saved to: ../logs/finetuned_v5_pretrain_ResMicroDB_90338_GATED_v9_5000/finetune_log.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4616</th>\n",
       "      <td>0.0730</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>32.99</td>\n",
       "      <td>22925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617</th>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>32.99</td>\n",
       "      <td>22930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4618</th>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>33.00</td>\n",
       "      <td>22935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4619</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.00</td>\n",
       "      <td>22935</td>\n",
       "      <td>0.174353</td>\n",
       "      <td>16.7844</td>\n",
       "      <td>662.221</td>\n",
       "      <td>10.367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4620</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.00</td>\n",
       "      <td>22935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7855.669</td>\n",
       "      <td>5659.607</td>\n",
       "      <td>88.471</td>\n",
       "      <td>2.977477e+16</td>\n",
       "      <td>0.137506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  learning_rate  epoch   step  eval_loss  eval_runtime  \\\n",
       "4616  0.0730        0.00001  32.99  22925        NaN           NaN   \n",
       "4617  0.0663        0.00001  32.99  22930        NaN           NaN   \n",
       "4618  0.0517        0.00001  33.00  22935        NaN           NaN   \n",
       "4619     NaN            NaN  33.00  22935   0.174353       16.7844   \n",
       "4620     NaN            NaN  33.00  22935        NaN           NaN   \n",
       "\n",
       "      eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "4616                      NaN                    NaN            NaN   \n",
       "4617                      NaN                    NaN            NaN   \n",
       "4618                      NaN                    NaN            NaN   \n",
       "4619                  662.221                 10.367            NaN   \n",
       "4620                      NaN                    NaN       7855.669   \n",
       "\n",
       "      train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "4616                       NaN                     NaN           NaN   \n",
       "4617                       NaN                     NaN           NaN   \n",
       "4618                       NaN                     NaN           NaN   \n",
       "4619                       NaN                     NaN           NaN   \n",
       "4620                  5659.607                  88.471  2.977477e+16   \n",
       "\n",
       "      train_loss  \n",
       "4616         NaN  \n",
       "4617         NaN  \n",
       "4618         NaN  \n",
       "4619         NaN  \n",
       "4620    0.137506  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start training...\")\n",
    "\n",
    "# ========= 1) 解决：train_subset / val_subset 可能是“嵌套 Subset”，需要解析到最底层 base dataset =========\n",
    "def resolve_subset_to_base_and_indices(ds):\n",
    "    \"\"\"\n",
    "    把 Dataset/Subset 递归展开，返回：\n",
    "      base_dataset: 最底层数据集（通常应是 all_corpus）\n",
    "      base_indices: ds 在 base_dataset 上的绝对索引\n",
    "    \"\"\"\n",
    "    if not isinstance(ds, Subset):\n",
    "        return ds, np.arange(len(ds), dtype=int)\n",
    "\n",
    "    base, base_idx = resolve_subset_to_base_and_indices(ds.dataset)\n",
    "    cur_idx = np.asarray(ds.indices, dtype=int)\n",
    "    return base, base_idx[cur_idx]\n",
    "\n",
    "# 解析 train/val 到最底层 dataset + 绝对索引\n",
    "base_train, train_idx = resolve_subset_to_base_and_indices(train_subset)\n",
    "base_val,   val_idx   = resolve_subset_to_base_and_indices(val_subset)\n",
    "\n",
    "# 一般情况下两者都会指向同一个 base（通常是 all_corpus）\n",
    "assert base_train is base_val, \"train/val 的 base dataset 不一致，这通常不应该发生。\"\n",
    "base_corpus = base_train\n",
    "\n",
    "print(\"train_idx range sample:\", train_idx[:10])\n",
    "print(\"val_idx range sample:\", val_idx[:10])\n",
    "\n",
    "# ========= 2) 取原始标签（从 metadata 里拿），再用同一个 le 编码成 0..C-1 =========\n",
    "train_raw = base_corpus.metadata.iloc[train_idx][args.label_col]\n",
    "val_raw   = base_corpus.metadata.iloc[val_idx][args.label_col]\n",
    "\n",
    "# 你已经在 finetune_subset 里过滤过 notna，这里再兜底检查一次\n",
    "assert train_raw.notna().all(), \"train 中仍存在 NA 标签，请检查你的 subset 条件。\"\n",
    "assert val_raw.notna().all(),   \"val 中仍存在 NA 标签，请检查你的 subset 条件。\"\n",
    "\n",
    "# 用同一个 label encoder（le）做 transform，保证 train/val 编码一致\n",
    "train_labels = le.transform(train_raw.tolist())\n",
    "val_labels   = le.transform(val_raw.tolist())\n",
    "\n",
    "# 你之前的断言（现在标签不再是 -1，而是 0..C-1）\n",
    "assert (train_labels >= 0).all()\n",
    "assert (val_labels >= 0).all()\n",
    "\n",
    "# ========= 3) 构建 Trainer 可用的 Dataset（沿用你原来的 FinetuneDataset 风格） =========\n",
    "train_dataset = FinetuneDataset(base_corpus, train_idx, train_labels)\n",
    "val_dataset   = FinetuneDataset(base_corpus, val_idx,   val_labels)\n",
    "\n",
    "# ========= 4) callbacks + Trainer =========\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=callbacks,\n",
    "\n",
    "    # ⚠️ 关键：不要传 tokenizer（否则可能触发 tokenizer.save_pretrained -> NotImplementedError）\n",
    "    tokenizer=None,\n",
    "\n",
    "    # 你的样本本身就是 tensor + 定长，默认 collator 最稳\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ========= 5) 保存模型 =========\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "trainer.save_model(args.output)\n",
    "\n",
    "# 保存 label encoder（你原来怎么做就怎么做）\n",
    "dump(le, open(os.path.join(args.output, \"label_encoder.pkl\"), \"wb\"))\n",
    "print(f\"Model and label encoder saved to: {args.output}\")\n",
    "\n",
    "# ========= 6) 保存日志 =========\n",
    "logs = trainer.state.log_history\n",
    "logs_df = pd.DataFrame(logs)\n",
    "\n",
    "os.makedirs(args.log, exist_ok=True)\n",
    "log_path = os.path.join(args.log, \"finetune_log.csv\")\n",
    "logs_df.to_csv(log_path, index=False)\n",
    "\n",
    "print(f\"Training logs saved to: {log_path}\")\n",
    "logs_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b6468-9686-4bef-8ad8-e373eb1fc799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (caiqy_MiCoSeq_dev)",
   "language": "python",
   "name": "caiqy_micoseq_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
