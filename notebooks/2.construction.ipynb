{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4b2211",
   "metadata": {},
   "source": [
    "## 0. å¯¼å…¥ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f665a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser          # è¯»å– config.ini\n",
    "from pathlib import Path                       # å¤„ç†è·¯å¾„\n",
    "from pickle import dump                        # ä¿å­˜æ„å»ºå¥½çš„ MicroCorpus\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # ä» notebooks/ å›åˆ°å¤–é¢çš„ MiCoGPT æ ¹ç›®å½•\n",
    "from MiCoGPT.utils.corpus import MicroCorpus\n",
    "from MiCoGPT.utils.base import CustomUnpickler\n",
    "\n",
    "# è®¾ç½®æ ¹ç›®å½•\n",
    "project_root = Path(\"/Users/kiancai/STA24/CWD/STAi/MiCoGPT\")\n",
    "\n",
    "# ç”Ÿæˆè·¯å¾„\n",
    "input_path = project_root / \"data\" / \"try2_withCC/abundance_A_subgroup_55575.csv\"\n",
    "output_path = project_root / \"data\" / \"try2_withCC/abundance_A_subgroup_55575.pkl\"\n",
    "config_path = project_root / \"MiCoGPT\" / \"resources\" / \"mgm_config.ini\"\n",
    "tokenizer_path = project_root / \"MiCoGPT\" / \"resources\" / \"MiCoGPTokenizer.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648c8cd",
   "metadata": {},
   "source": [
    "## 1. å®šä¹‰ construct_simple å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05dad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_simple(\n",
    "    input_path: str,       # è¾“å…¥ä¸°åº¦è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆh5 / csv / tsv / txtï¼‰\n",
    "    output_path: str,      # è¾“å‡ºçš„ MicroCorpus pkl è·¯å¾„\n",
    "    config_path: str,      # config.ini çš„è·¯å¾„\n",
    "    tokenizer_path: str,   # MicroTokenizer.pkl çš„è·¯å¾„\n",
    "    key: str = \"genus\",\n",
    "    no_normalize: bool = False,  # æ˜¯å¦è·³è¿‡å½’ä¸€åŒ–\n",
    "):\n",
    "\n",
    "    # config.iniï¼Œæ‹¿åˆ° max_len\n",
    "    cfg = ConfigParser()\n",
    "    cfg.read(config_path)\n",
    "    max_len = cfg.getint(\"construct\", \"max_len\")\n",
    "\n",
    "    print(f\"[construct] ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–åˆ° max_len = {max_len}\")\n",
    "\n",
    "\n",
    "    # åŠ è½½ MicroTokenizer.pkl\n",
    "    tokenizer_path = Path(tokenizer_path)\n",
    "    if not tokenizer_path.is_file():\n",
    "        raise FileNotFoundError(f\"æ‰¾ä¸åˆ° tokenizer æ–‡ä»¶ï¼š{tokenizer_path}\")\n",
    "\n",
    "    with open(tokenizer_path, \"rb\") as f:\n",
    "        # ç”¨è‡ªå®šä¹‰çš„ CustomUnpickler æ¥ååºåˆ—åŒ–ï¼Œæ˜¯ä¸ºäº†åœ¨ä¸åŒç¯å¢ƒä¸‹\n",
    "        # ä¹Ÿèƒ½æ­£ç¡®æ‰¾åˆ° MicroTokenizer ç±»æ‰€åœ¨çš„æ¨¡å—ã€‚\n",
    "        unpickler = CustomUnpickler(f)\n",
    "        tokenizer = unpickler.load()\n",
    "\n",
    "    # é¡ºä¾¿æ‰“å°ä¸€ç‚¹ä¿¡æ¯å¸®åŠ©ç†è§£ tokenizer\n",
    "    try:\n",
    "        vocab_size = len(tokenizer.vocab)\n",
    "        print(f\"[construct] tokenizer åŠ è½½å®Œæˆï¼Œè¯è¡¨å¤§å° = {vocab_size}\")\n",
    "    except Exception:\n",
    "        print(\"[construct] tokenizer å·²åŠ è½½ï¼ˆæœªè®¿é—® vocabï¼Œå¯èƒ½å®ç°ä¸åŒï¼‰\")\n",
    "\n",
    "    # æ‰“å°å½’ä¸€åŒ–æç¤º\n",
    "    if not no_normalize:\n",
    "        print(\n",
    "            \"Your data will be normalized with the phylogeny mean and std. \"\n",
    "            \"If you wish to use your own normalization, please set no_normalize=True.\"\n",
    "        )\n",
    "\n",
    "\n",
    "    #  æ„å»º MicroCorpus\n",
    "    input_path = Path(input_path)\n",
    "    if not input_path.is_file():\n",
    "        raise FileNotFoundError(f\"æ‰¾ä¸åˆ°è¾“å…¥ä¸°åº¦è¡¨æ–‡ä»¶ï¼š{input_path}\")\n",
    "\n",
    "    corpus = MicroCorpus(\n",
    "        data_path=str(input_path),\n",
    "        tokenizer=tokenizer,\n",
    "        key=key,\n",
    "        max_len=max_len,\n",
    "        preprocess=not no_normalize,\n",
    "    )\n",
    "\n",
    "    print(\"[construct] MicroCorpus å·²æ„å»ºå®Œæˆã€‚\")\n",
    "    try:\n",
    "        print(f\"[construct] è¯­æ–™åº“æ ·æœ¬æ•°é‡ = {len(corpus)}\")\n",
    "    except TypeError:\n",
    "        print(\"[construct] corpus ä¸æ”¯æŒ len()ï¼Œå¯ä»¥è‡ªè¡Œæ£€æŸ¥å†…éƒ¨å®ç°ã€‚\")\n",
    "\n",
    "\n",
    "    # ä¿å­˜åˆ° output_path\n",
    "    output_path = Path(output_path)\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        dump(corpus, f)\n",
    "\n",
    "    print(f\"[construct] è¯­æ–™åº“å·²ä¿å­˜åˆ°ï¼š{output_path.resolve()}\")\n",
    "\n",
    "    # è¿”å› corpusï¼Œæ–¹ä¾¿åç»­åœ¨ Notebook é‡Œåˆ†æ\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7c023",
   "metadata": {},
   "source": [
    "## 2. æ‰§è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f00e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[construct] ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–åˆ° max_len = 512\n",
      "[construct] tokenizer åŠ è½½å®Œæˆï¼Œè¯è¡¨å¤§å° = 1121\n",
      "Your data will be normalized with the phylogeny mean and std. If you wish to use your own normalization, please set no_normalize=True.\n",
      "0 samples are dropped for all zeroes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55575/55575 [00:07<00:00, 7770.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 55575 samples.\n",
      "            Max length is 701.\n",
      "            Average length is 55.86652271704903.\n",
      "            Min length is 4.\n",
      "[construct] MicroCorpus å·²æ„å»ºå®Œæˆã€‚\n",
      "[construct] è¯­æ–™åº“æ ·æœ¬æ•°é‡ = 55575\n",
      "[construct] è¯­æ–™åº“å·²ä¿å­˜åˆ°ï¼š/Users/kiancai/STA24/CWD/STAi/ResMicroDb/data/try2_withCC/abundance_A_subgroup_55575.pkl\n"
     ]
    }
   ],
   "source": [
    "corpus = construct_simple(\n",
    "    input_path=str(input_path),\n",
    "    output_path=str(output_path),\n",
    "    config_path=str(config_path),\n",
    "    tokenizer_path=str(tokenizer_path),\n",
    "    key=\"genus\",\n",
    "    no_normalize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1fb6f",
   "metadata": {},
   "source": [
    "## 3. æ£€æŸ¥ç”Ÿæˆçš„ç»“æœç»“æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f611351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š [æ¦‚è§ˆ] æ ·æœ¬æ€»æ•°: 55575 | æœ€çŸ­é•¿åº¦: 4 | æœ€é•¿é•¿åº¦: 512\n",
      "\n",
      "========================================\n",
      "ğŸ§ ğŸŸ¢ æœ€çŸ­ (Shortest) | Index: 1439 | é•¿åº¦: 4/512\n",
      "----------------------------------------\n",
      "Pos   ID     Token\n",
      "0     1119   <bos>\n",
      "1     396    g__Pseudomonas\n",
      "2     481    g__Streptococcus\n",
      "3     1120   <eos>\n",
      "4     0      <pad>\n",
      "5     0      <pad>\n",
      "6     0      <pad>\n",
      "...   ...    ...\n",
      "510   0      <pad>\n",
      "511   0      <pad>\n",
      "\n",
      "========================================\n",
      "ğŸ§ ğŸ”´ æœ€é•¿ (Longest)  | Index: 23241 | é•¿åº¦: 512/512\n",
      "----------------------------------------\n",
      "Pos   ID     Token\n",
      "0     1119   <bos>\n",
      "1     715    g__Oceanobacillus\n",
      "2     665    g__Pseudogracilibacillus\n",
      "3     266    g__Tissierella\n",
      "4     85     g__Methyloligellaceae\n",
      "...   ...    ...\n",
      "510   189    g__Gemella\n",
      "511   732    g__Fusobacterium\n",
      "\n",
      "========================================\n",
      "ğŸš€ è¯è¡¨åˆ©ç”¨ç‡è¯Šæ–­\n",
      "========================================\n",
      "ğŸ’¡ è¯è¡¨åˆ©ç”¨ç‡: 1117/1121 (99.6%)\n",
      "â“ æœªå‡ºç°çš„èŒå±æ•°é‡: 0\n",
      "âœ… å®Œç¾ï¼æ‰€æœ‰èŒå±éƒ½å·²åŒ…å« (åˆ©ç”¨ç‡ç¬¦åˆé¢„æœŸ)ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiancai/STA24/CWD/STAi/MiCoGPT/MiCoGPT/utils/corpus.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # ================= é…ç½®åŒºåŸŸ =================\n",
    "    # è¯·ç¡®è®¤è¿™æ˜¯ä½ ç”¨æ¥æ„å»º corpus çš„åŸå§‹ CSV è·¯å¾„\n",
    "    csv_path = \"/Users/kiancai/STA24/CWD/STAi/MiCoGPT/data/try2_withCC/abundance_B_13901.csv\"\n",
    "    # ===========================================\n",
    "\n",
    "    # --- 1. åŸºç¡€æ¦‚è§ˆä¸æ ·æœ¬æ£€æŸ¥ (ä»…æŸ¥çœ‹æœ€çŸ­å’Œæœ€é•¿) ---\n",
    "    pad_id = corpus.tokenizer.pad_token_id\n",
    "    real_lengths = (corpus.tokens != pad_id).sum(dim=1)\n",
    "\n",
    "    targets = [\n",
    "        (\"ğŸŸ¢ æœ€çŸ­ (Shortest)\", torch.argmin(real_lengths).item()),\n",
    "        (\"ğŸ”´ æœ€é•¿ (Longest) \", torch.argmax(real_lengths).item())\n",
    "    ]\n",
    "\n",
    "    print(f\"ğŸ“Š [æ¦‚è§ˆ] æ ·æœ¬æ€»æ•°: {len(corpus)} | æœ€çŸ­é•¿åº¦: {real_lengths.min()} | æœ€é•¿é•¿åº¦: {real_lengths.max()}\")\n",
    "\n",
    "    for title, idx in targets:\n",
    "        ids = corpus[idx]['input_ids']\n",
    "        valid_len = real_lengths[idx].item()\n",
    "        \n",
    "        print(f\"\\n{'='*40}\\nğŸ§ {title} | Index: {idx} | é•¿åº¦: {valid_len}/{len(ids)}\\n{'-'*40}\")\n",
    "        print(f\"{'Pos':<5} {'ID':<6} {'Token'}\")\n",
    "        \n",
    "        # æ™ºèƒ½åˆ‡ç‰‡ï¼šå¼€å¤´5ä¸ª ... ä¸­é—´(EOSé™„è¿‘) ... ç»“å°¾\n",
    "        indices = sorted(list(set(\n",
    "            list(range(0, 5)) + \n",
    "            list(range(valid_len - 2, valid_len + 3)) + \n",
    "            list(range(len(ids) - 2, len(ids)))\n",
    "        )))\n",
    "\n",
    "        last_i = -1\n",
    "        for i in indices:\n",
    "            if i >= len(ids): continue\n",
    "            if last_i != -1 and i > last_i + 1:\n",
    "                print(f\"{'...':<5} {'...':<6} ...\")\n",
    "            \n",
    "            token = corpus.tokenizer.convert_ids_to_tokens(ids[i].item())\n",
    "            print(f\"{i:<5} {ids[i]:<6} {token}\")\n",
    "            last_i = i\n",
    "\n",
    "    # --- 2. è¯è¡¨åˆ©ç”¨ç‡ä¸ä¸¢å¤±è¯Šæ–­ ---\n",
    "    print(f\"\\n{'='*40}\\nğŸš€ è¯è¡¨åˆ©ç”¨ç‡è¯Šæ–­\\n{'='*40}\")\n",
    "\n",
    "    # ç»Ÿè®¡å½“å‰ Corpus ä½¿ç”¨æƒ…å†µ\n",
    "    unique_ids = set(torch.unique(corpus.tokens).tolist())\n",
    "    all_vocab_ids = set(range(corpus.tokenizer.vocab_size))\n",
    "    special_ids = set(corpus.tokenizer.convert_tokens_to_ids(['<pad>', '<bos>', '<eos>', '<mask>']))\n",
    "    target_vocab_ids = all_vocab_ids - special_ids # ç†è®ºä¸Šåº”æœ‰çš„èŒå±ID\n",
    "\n",
    "    used_vocab = len(unique_ids - special_ids)\n",
    "    missing_ids = target_vocab_ids - unique_ids\n",
    "    missing_genera = [corpus.tokenizer.convert_ids_to_tokens(i) for i in missing_ids]\n",
    "\n",
    "    print(f\"ğŸ’¡ è¯è¡¨åˆ©ç”¨ç‡: {used_vocab}/{len(corpus.tokenizer)} ({used_vocab/len(corpus.tokenizer):.1%})\")\n",
    "    print(f\"â“ æœªå‡ºç°çš„èŒå±æ•°é‡: {len(missing_genera)}\")\n",
    "\n",
    "    # å¦‚æœæœ‰ä¸¢å¤±ï¼Œè¯»å– CSV è¿›è¡Œæ¯”å¯¹ (ä½¿ç”¨ä¿®æ­£åçš„æ­£åˆ™)\n",
    "    if len(missing_genera) > 0:\n",
    "        print(f\"\\nğŸ“– æ­£åœ¨è¯»å– CSV æ ¸å¯¹: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df_raw = pd.read_csv(csv_path, index_col=0)\n",
    "            # å…³é”®ä¿®æ­£ï¼šä½¿ç”¨æ›´å®½å®¹çš„æ­£åˆ™æå–åˆ—åï¼Œç¡®ä¿èƒ½åŒ¹é…åˆ°å¸¦ç‰¹æ®Šå­—ç¬¦çš„èŒå\n",
    "            raw_genera = set(df_raw.columns.str.extract(r'(g__[^;]+)').squeeze().tolist())\n",
    "            \n",
    "            not_in_csv = [g for g in missing_genera if g not in raw_genera]\n",
    "            in_csv_but_zero = [g for g in missing_genera if g in raw_genera]\n",
    "            \n",
    "            if not_in_csv:\n",
    "                print(f\"\\nâŒ [ä¸¥é‡] {len(not_in_csv)} ä¸ªèŒåœ¨ CSV è¡¨å¤´ä¸­å®Œå…¨æ‰¾ä¸åˆ°:\")\n",
    "                print(f\"   (å¦‚æœæ•°é‡å¾ˆå¤šï¼Œè¯·æ£€æŸ¥ corpus.py ä¸­çš„æ­£åˆ™æ˜¯å¦å·²ä¿®æ”¹ä¸º r'(g__[^;]+)')\")\n",
    "                print(f\"   ç¤ºä¾‹: {not_in_csv[:5]} ...\")\n",
    "            \n",
    "            if in_csv_but_zero:\n",
    "                print(f\"\\nğŸ“‰ [æ­£å¸¸] {len(in_csv_but_zero)} ä¸ªèŒåœ¨ CSV ä¸­å­˜åœ¨ï¼Œä½†åœ¨è¯­æ–™åº“ä¸­æ¶ˆå¤±:\")\n",
    "                print(f\"   åŸå› : è¿™äº›èŒåœ¨æ‰€æœ‰æ ·æœ¬ä¸­çš„ä¸°åº¦å¯èƒ½å‡ä¸º 0ï¼Œæˆ–è€…è¢« Phylogeny æ ‘è¿‡æ»¤äº†ã€‚\")\n",
    "                print(f\"   ç¤ºä¾‹: {in_csv_but_zero[:5]} ...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯»å– CSV å¤±è´¥: {e}\")\n",
    "    else:\n",
    "        print(\"âœ… å®Œç¾ï¼æ‰€æœ‰èŒå±éƒ½å·²åŒ…å« (åˆ©ç”¨ç‡ç¬¦åˆé¢„æœŸ)ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08a6b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š å¼€å§‹ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„çœŸå®è¯è¡¨é•¿åº¦...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55575/55575 [00:07<00:00, 7548.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ç»Ÿè®¡å®Œæˆï¼CSV å·²ä¿å­˜åˆ°: /Users/kiancai/STA24/CWD/STAi/MiCoGPT/data/try2_withCC/sample_length_stats_B.csv\n",
      "\n",
      "--- æ‘˜è¦ ---\n",
      "æ€»æ ·æœ¬æ•°: 55575\n",
      "è¢«æˆªæ–­çš„æ ·æœ¬æ•°: 45 (0.08%)\n",
      "æœ€ä¸¥é‡çš„æˆªæ–­: ä¸¢æ‰äº† 189 ä¸ªè¯\n",
      "\n",
      "é¢„è§ˆç”Ÿæˆçš„è¡¨æ ¼:\n",
      "   Sample_ID  Original_Length  Max_Limit  Truncated_Tokens  Is_Truncated\n",
      "0  CRR768228              132        512                 0         False\n",
      "1  CRR768229              144        512                 0         False\n",
      "2  CRR768230              195        512                 0         False\n",
      "3  CRR768231              198        512                 0         False\n",
      "4  CRR768232              124        512                 0         False\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # ================= é…ç½® =================\n",
    "    save_path = project_root / \"data\" / \"try2_withCC/sample_length_stats_B.csv\"\n",
    "    # =======================================\n",
    "\n",
    "    print(\"ğŸ“Š å¼€å§‹ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„çœŸå®è¯è¡¨é•¿åº¦...\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # æ£€æŸ¥ corpus æ˜¯å¦ä¿ç•™äº†åŸå§‹æ•°æ®\n",
    "    if not hasattr(corpus, 'data') or corpus.data is None:\n",
    "        print(\"âŒ é”™è¯¯: corpus å¯¹è±¡ä¸­æ²¡æœ‰æ‰¾åˆ° .data å±æ€§ã€‚\")\n",
    "        print(\"å¯èƒ½æ˜¯å› ä¸ºæ„å»ºæ—¶ä¸ºäº†çœå†…å­˜æ‰§è¡Œäº† del self.dataã€‚\")\n",
    "        print(\"å¦‚æœè¿™æ ·ï¼Œä½ éœ€è¦é‡æ–°åŠ è½½ CSV åˆ°å˜é‡ (ä¾‹å¦‚ df_raw) å¹¶åœ¨æ­¤å¤„éå†å®ƒã€‚\")\n",
    "    else:\n",
    "        # éå† corpus å†…éƒ¨å­˜å‚¨çš„åŸå§‹ dataframe\n",
    "        # corpus.data çš„ç´¢å¼•é€šå¸¸å°±æ˜¯ Sample ID\n",
    "        for sample_id in tqdm(corpus.data.index, desc=\"Analyzing\"):\n",
    "            \n",
    "            # 1. è·å–è¯¥æ ·æœ¬çš„ä¸€è¡Œæ•°æ®\n",
    "            row_data = corpus.data.loc[sample_id]\n",
    "            \n",
    "            # 2. è°ƒç”¨å†…éƒ¨å‡½æ•°è®¡ç®—é•¿åº¦\n",
    "            # _convert_to_token è¿”å›: (å¤„ç†åçš„tokenåˆ—è¡¨, åŸå§‹é•¿åº¦)\n",
    "            # æ³¨æ„ï¼šè¿™ä¸ª original_len æ˜¯åŒ…å« <bos> å’Œ <eos> çš„\n",
    "            _, original_len = corpus._convert_to_token(row_data)\n",
    "            \n",
    "            # 3. è®¡ç®—æˆªæ–­æƒ…å†µ\n",
    "            max_len = corpus.max_len\n",
    "            # å®é™…ä¿ç•™çš„æœ‰æ•ˆé•¿åº¦ (ä¸å« pad)\n",
    "            kept_len = min(original_len, max_len)\n",
    "            # è¢«ä¸¢å¼ƒçš„è¯æ•°é‡\n",
    "            truncated_count = max(0, original_len - max_len)\n",
    "            \n",
    "            results.append({\n",
    "                \"Sample_ID\": sample_id,\n",
    "                \"Original_Length\": original_len,   # åŸå§‹çœŸå®é•¿åº¦\n",
    "                \"Max_Limit\": max_len,              # è®¾å®šçš„ä¸Šé™ (512)\n",
    "                \"Truncated_Tokens\": truncated_count, # è¢«æˆªæ–­ä¸¢æ‰çš„è¯æ•°\n",
    "                \"Is_Truncated\": truncated_count > 0  # æ˜¯å¦å‘ç”Ÿäº†æˆªæ–­\n",
    "            })\n",
    "\n",
    "        # 4. ç”Ÿæˆ DataFrame å¹¶ä¿å­˜\n",
    "        df_stats = pd.DataFrame(results)\n",
    "        df_stats.to_csv(save_path, index=False)\n",
    "\n",
    "        print(f\"\\nâœ… ç»Ÿè®¡å®Œæˆï¼CSV å·²ä¿å­˜åˆ°: {save_path}\")\n",
    "        \n",
    "        # 5. æ‰“å°ä¸€äº›æ‘˜è¦ä¿¡æ¯\n",
    "        trunc_num = df_stats['Is_Truncated'].sum()\n",
    "        print(f\"\\n--- æ‘˜è¦ ---\")\n",
    "        print(f\"æ€»æ ·æœ¬æ•°: {len(df_stats)}\")\n",
    "        print(f\"è¢«æˆªæ–­çš„æ ·æœ¬æ•°: {trunc_num} ({trunc_num/len(df_stats):.2%})\")\n",
    "        if trunc_num > 0:\n",
    "            max_loss = df_stats['Truncated_Tokens'].max()\n",
    "            print(f\"æœ€ä¸¥é‡çš„æˆªæ–­: ä¸¢æ‰äº† {max_loss} ä¸ªè¯\")\n",
    "            \n",
    "        # çœ‹çœ‹å‰å‡ è¡Œ\n",
    "        print(\"\\né¢„è§ˆç”Ÿæˆçš„è¡¨æ ¼:\")\n",
    "        print(df_stats.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
