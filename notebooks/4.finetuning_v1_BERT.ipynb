{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5325b9",
   "metadata": {},
   "source": [
    "## 0. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca57f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pickle import load, dump\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import (\n",
    "    GPT2ForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "from configparser import ConfigParser\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e568c13",
   "metadata": {},
   "source": [
    "## 1. 加载 / 设置配置（cfg）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2336c6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[finetune] section:\n",
      "learning_rate = 1e-3\n",
      "warmup_steps = 100\n",
      "weight_decay = 0.001\n",
      "per_device_train_batch_size = 64\n",
      "num_train_epochs = 1000\n",
      "logging_steps = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(input='../data/try2_withCC/ResMicroDB_90338.pkl', model='../models/pretrain_ResMicroDB_90338_BERT', output='../models/finetuned_model_ResMicroDB_90338_BERT', log='../logs/finetuned_ResMicroDB_90338_BERT', val_split=0.2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_path = \"../MiCoGPT/resources/mgm_config.ini\"\n",
    "\n",
    "cfg = ConfigParser()\n",
    "cfg.read(cfg_path)\n",
    "\n",
    "print(\"[finetune] section:\")\n",
    "for k, v in cfg[\"finetune\"].items():\n",
    "    print(f\"{k} = {v}\")\n",
    "\n",
    "input_corpus_path = \"../data/try2_withCC/ResMicroDB_90338.pkl\"\n",
    "pretrained_model_path = \"../models/pretrain_ResMicroDB_90338_BERT\"\n",
    "output_model_dir  = \"../models/finetuned_model_ResMicroDB_90338_BERT\"\n",
    "log_dir           = \"../logs/finetuned_ResMicroDB_90338_BERT\"\n",
    "val_split         = 0.2         # 验证集比例\n",
    "\n",
    "args = Namespace(\n",
    "    input=input_corpus_path,\n",
    "    model=pretrained_model_path,\n",
    "    output=output_model_dir,\n",
    "    log=log_dir,\n",
    "    val_split=val_split,\n",
    ")\n",
    "\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b01b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in all_corpus: 90338\n",
      "Samples used for finetune: 55575\n",
      "Label distribution (A group, non-NA):\n",
      "Is_Healthy\n",
      "False    31073\n",
      "True     24502\n",
      "Name: count, dtype: int64\n",
      "corpus size: 55575\n",
      "labels size: 55575\n",
      "First few labels:\n",
      "           Is_Healthy\n",
      "Run                 \n",
      "CRR768228      False\n",
      "CRR768229      False\n",
      "CRR768230      False\n",
      "CRR768231      False\n",
      "CRR768232      False\n",
      "num_labels: 2\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 1. 载入大 corpus，并筛选出要用于 finetune 的子集\n",
    "all_corpus = load(open(args.input, \"rb\"))\n",
    "tokenizer = all_corpus.tokenizer\n",
    "\n",
    "# 子集：Split_Group == \"A\" 且 Is_Healthy 非 NA\n",
    "corpus = all_corpus.subset_by_metadata(\n",
    "    lambda df: (df[\"Split_Group\"] == \"A\") & df[\"Is_Healthy\"].notna()\n",
    ")\n",
    "\n",
    "print(\"Total samples in all_corpus:\", len(all_corpus))\n",
    "print(\"Samples used for finetune:\", len(corpus))\n",
    "\n",
    "meta = all_corpus.metadata\n",
    "mask = (meta[\"Split_Group\"] == \"A\") & meta[\"Is_Healthy\"].notna()\n",
    "print(\"Label distribution (A group, non-NA):\")\n",
    "print(meta.loc[mask, \"Is_Healthy\"].value_counts())\n",
    "\n",
    "# 2. 从 metadata 中提取标签（基于 all_corpus）\n",
    "labels_series = meta.loc[mask, \"Is_Healthy\"]   # index = sample_id\n",
    "\n",
    "# 3. 将标签按 Subset corpus 的顺序对齐\n",
    "indices = np.array(corpus.indices)   # 这些是 all_corpus 中的行号\n",
    "sample_ids = np.array(all_corpus.sample_ids)[indices]\n",
    "\n",
    "# 按 sample_id 顺序取标签，确保顺序和 corpus.__getitem__ 一致\n",
    "labels = labels_series.loc[sample_ids]\n",
    "labels = labels.to_frame(name=\"Is_Healthy\")\n",
    "\n",
    "print(\"corpus size:\", len(corpus))\n",
    "print(\"labels size:\", len(labels))\n",
    "print(\"First few labels:\\n\", labels.head())\n",
    "\n",
    "# 4. label encoding（保持原来的 OneHotEncoder 流程）\n",
    "le = OneHotEncoder()\n",
    "labels_arr = le.fit_transform(labels.values.reshape(-1, 1)).toarray()\n",
    "labels_tensor = torch.tensor(labels_arr.argmax(axis=1), dtype=torch.long)\n",
    "\n",
    "num_labels = len(le.categories_[0])\n",
    "print(\"num_labels:\", num_labels)\n",
    "\n",
    "# 5. 构造一个“全局标签数组”：和 all_corpus 对齐，其他位置填 -1\n",
    "all_labels = np.full(len(all_corpus), fill_value=-1, dtype=int)\n",
    "# corpus.indices 这部分是 finetune 子集，对应的标签是 labels_tensor\n",
    "all_labels[indices] = labels_tensor.numpy()\n",
    "\n",
    "# 简单 sanity check：在 finetune 子集的位置不应该再有 -1\n",
    "assert (all_labels[indices] != -1).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b01364ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ../models/pretrain_ResMicroDB_90338_BERT and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=True,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=True,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=0.001,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=../logs/finetuned_ResMicroDB_90338_BERT,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=5,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1000,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "output_dir=../logs/finetuned_ResMicroDB_90338_BERT/finetune_checkpoints,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=64,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=../logs/finetuned_ResMicroDB_90338_BERT/finetune_checkpoints,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=100,\n",
       "weight_decay=0.001,\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model config\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    args.model,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "model\n",
    "\n",
    "training_args_dict = {\n",
    "    \"learning_rate\": cfg.getfloat(\"finetune\", \"learning_rate\"),\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": False,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"warmup_steps\": cfg.getint(\"finetune\", \"warmup_steps\"),\n",
    "    \"weight_decay\": cfg.getfloat(\"finetune\", \"weight_decay\"),\n",
    "    \"per_device_train_batch_size\": cfg.getint(\"finetune\", \"per_device_train_batch_size\"),\n",
    "    \"num_train_epochs\": cfg.getint(\"finetune\", \"num_train_epochs\"),\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_steps\": cfg.getint(\"finetune\", \"logging_steps\"),\n",
    "    \"output_dir\": f\"{args.log}/finetune_checkpoints\",\n",
    "    \"logging_dir\": args.log,\n",
    "    \"load_best_model_at_end\": True,\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(**training_args_dict)\n",
    "training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a629708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def split_train_val_by_project(dataset, val_ratio=0.1, project_col=\"Project_ID\", random_state=42):\n",
    "    \"\"\"\n",
    "    按 project 划分 train / val：\n",
    "    - 支持传入 MiCoGPTCorpus 或它的 Subset\n",
    "    - 在“当前 dataset 所包含的样本集合”上，按 project 划分\n",
    "    - 选出若干个 project 作为验证集\n",
    "    - 这些 project 的样本总数 ≈ val_ratio * 当前 dataset 的样本数\n",
    "    - 同一个 project 只会出现在 train 或 val 其中之一\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 识别当前传入的是 corpus 本体还是 Subset\n",
    "    if isinstance(dataset, Subset):\n",
    "        base_corpus = dataset.dataset                    # 真正的 MiCoGPTCorpus\n",
    "        base_indices = np.array(dataset.indices)         # 当前子集对应的“在 base_corpus 中的行号”\n",
    "    else:\n",
    "        base_corpus = dataset\n",
    "        base_indices = np.arange(len(dataset))           # 整个 corpus 的所有行号\n",
    "\n",
    "    # 2. 在 base_corpus.metadata 中取出“当前子集部分”的 metadata\n",
    "    meta_full = base_corpus.metadata\n",
    "    if meta_full is None:\n",
    "        raise ValueError(\"base_corpus.metadata 为空，无法按 Project_ID 划分。\")\n",
    "\n",
    "    if project_col not in meta_full.columns:\n",
    "        raise ValueError(f\"metadata 中没有列 '{project_col}'，请检查列名。\")\n",
    "\n",
    "    # 只看当前子集的 metadata\n",
    "    meta = meta_full.iloc[base_indices].copy()\n",
    "    n_samples = meta.shape[0]\n",
    "    target_val = int(n_samples * val_ratio)\n",
    "\n",
    "    # 3. 取出当前子集中所有 project_id（去掉缺失值）\n",
    "    project_ids = meta[project_col].to_numpy()\n",
    "    # 去除 NaN（如果有的话）\n",
    "    mask_not_nan = pd.notna(project_ids)\n",
    "    project_ids_nonan = project_ids[mask_not_nan]\n",
    "\n",
    "    unique_projects = np.unique(project_ids_nonan)\n",
    "\n",
    "    # 4. 打乱 project 顺序\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(unique_projects)\n",
    "\n",
    "    # 5. 建立一个样本级别的布尔数组：is_val[i] 表示第 i 个样本是否进验证集\n",
    "    is_val = np.zeros(n_samples, dtype=bool)\n",
    "    val_projects = []\n",
    "    val_count = 0\n",
    "\n",
    "    for pid in unique_projects:\n",
    "        if val_count >= target_val:\n",
    "            break\n",
    "\n",
    "        # 当前 project 对应的样本（在“当前子集中的局部索引”）\n",
    "        proj_mask = (project_ids == pid)\n",
    "        # 这个 project 在当前子集中有多少样本\n",
    "        proj_size = proj_mask.sum()\n",
    "        if proj_size == 0:\n",
    "            continue  # 理论上不会，但防御一下\n",
    "\n",
    "        # 把这个 project 全部丢进验证集\n",
    "        is_val |= proj_mask\n",
    "        val_projects.append(pid)\n",
    "        val_count += proj_size\n",
    "\n",
    "    # 6. 根据 is_val，映射回 base_corpus 的索引\n",
    "    val_base_indices = base_indices[is_val]\n",
    "    train_base_indices = base_indices[~is_val]\n",
    "\n",
    "    # 7. 构造最终子集\n",
    "    train_set = Subset(base_corpus, train_base_indices.tolist())\n",
    "    val_set   = Subset(base_corpus, val_base_indices.tolist())\n",
    "\n",
    "    # 8. 打印真正的数量（一定和 len(val_set) 一致）\n",
    "    print(\n",
    "        f\"[split_by_project] 选中 {len(val_projects)} 个 project 作为验证集，\"\n",
    "        f\"验证样本数 {len(val_set)}，目标约 {target_val}，当前 dataset 样本数 {n_samples}\"\n",
    "    )\n",
    "    print(f\"[split_by_project] Train samples: {len(train_set)}, Val samples: {len(val_set)}\")\n",
    "\n",
    "    return train_set, val_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98744451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FinetuneDataset(Dataset):\n",
    "    def __init__(self, base_corpus, indices, labels_array):\n",
    "        self.base_corpus = base_corpus\n",
    "        self.indices = np.array(indices)\n",
    "        self.labels = np.array(labels_array, dtype=int)\n",
    "\n",
    "        assert len(self.indices) == len(self.labels), \\\n",
    "            \"indices 和 labels_array 的长度必须一致\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_idx = int(self.indices[idx])\n",
    "        item = self.base_corpus[base_idx]  # {'input_ids', 'attention_mask'}\n",
    "\n",
    "        # 加 labels\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        # 为了配合 TrainingArguments 里的 group_by_length / length_column_name=\"length\"\n",
    "        # 用 attention_mask 中非 pad 的 token 数目当作长度\n",
    "        if \"attention_mask\" in item:\n",
    "            # attention_mask 是 0/1 tensor\n",
    "            item[\"length\"] = int(item[\"attention_mask\"].sum().item())\n",
    "        else:\n",
    "            # 保险：如果将来改了 __getitem__，至少不至于 KeyError\n",
    "            item[\"length\"] = int((item[\"input_ids\"] != 0).sum().item())\n",
    "\n",
    "        return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4157a3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[split_by_project] 选中 47 个 project 作为验证集，验证样本数 11272，目标约 11115，当前 dataset 样本数 55575\n",
      "[split_by_project] Train samples: 44303, Val samples: 11272\n",
      "Train samples (subset): 44303, Val samples (subset): 11272\n",
      "Train labels value counts: 0    25302\n",
      "1    19001\n",
      "Name: count, dtype: int64\n",
      "Val labels value counts: 0    5771\n",
      "1    5501\n",
      "Name: count, dtype: int64\n",
      "Train dataset size: 44303\n",
      "Val dataset size: 11272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7623' max='693000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7623/693000 40:43 < 61:02:46, 3.12 it/s, Epoch 11/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.821553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.931236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>1.005533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>1.060238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>1.078619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>1.227279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>1.629533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>1.255509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.099200</td>\n",
       "      <td>1.384293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.969523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>1.090912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n",
      "/home/cml_lab/caiqy/project/MiCoGPT/MiCoGPT/utils/corpus.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(tokens),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and label encoder saved to: ../models/finetuned_model_ResMicroDB_90338_BERT\n",
      "Training logs saved to: ../logs/finetuned_ResMicroDB_90338_BERT/finetune_log.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>10.98</td>\n",
       "      <td>7610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>0.1101</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>10.99</td>\n",
       "      <td>7615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>11.00</td>\n",
       "      <td>7620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.00</td>\n",
       "      <td>7623</td>\n",
       "      <td>1.090912</td>\n",
       "      <td>19.5115</td>\n",
       "      <td>577.709</td>\n",
       "      <td>72.214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.00</td>\n",
       "      <td>7623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2445.0091</td>\n",
       "      <td>18119.769</td>\n",
       "      <td>283.435</td>\n",
       "      <td>9.460248e+15</td>\n",
       "      <td>0.11982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  learning_rate  epoch  step  eval_loss  eval_runtime  \\\n",
       "1531  0.0750       0.000989  10.98  7610        NaN           NaN   \n",
       "1532  0.1101       0.000989  10.99  7615        NaN           NaN   \n",
       "1533  0.1260       0.000989  11.00  7620        NaN           NaN   \n",
       "1534     NaN            NaN  11.00  7623   1.090912       19.5115   \n",
       "1535     NaN            NaN  11.00  7623        NaN           NaN   \n",
       "\n",
       "      eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "1531                      NaN                    NaN            NaN   \n",
       "1532                      NaN                    NaN            NaN   \n",
       "1533                      NaN                    NaN            NaN   \n",
       "1534                  577.709                 72.214            NaN   \n",
       "1535                      NaN                    NaN      2445.0091   \n",
       "\n",
       "      train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "1531                       NaN                     NaN           NaN   \n",
       "1532                       NaN                     NaN           NaN   \n",
       "1533                       NaN                     NaN           NaN   \n",
       "1534                       NaN                     NaN           NaN   \n",
       "1535                 18119.769                 283.435  9.460248e+15   \n",
       "\n",
       "      train_loss  \n",
       "1531         NaN  \n",
       "1532         NaN  \n",
       "1533         NaN  \n",
       "1534         NaN  \n",
       "1535     0.11982  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start training...\")\n",
    "model = model.train()\n",
    "\n",
    "split = args.val_split  # 例如 0.2\n",
    "\n",
    "# 1. 在当前 finetune 子集（corpus）范围内，按 Project_ID 分组划分\n",
    "train_subset, val_subset = split_train_val_by_project(\n",
    "    corpus,\n",
    "    val_ratio=split,\n",
    "    project_col=\"Project_ID\",    # 如果列名不同，这里改掉\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Train samples (subset): {len(train_subset)}, Val samples (subset): {len(val_subset)}\")\n",
    "\n",
    "# 2. 提取对应的全局索引（在 all_corpus 中的位置）\n",
    "train_idx = np.array(train_subset.indices)\n",
    "val_idx   = np.array(val_subset.indices)\n",
    "\n",
    "# 3. 从全局标签数组中取出对应标签\n",
    "train_labels = all_labels[train_idx]\n",
    "val_labels   = all_labels[val_idx]\n",
    "\n",
    "# 做个 sanity check：这里不应该有 -1\n",
    "assert (train_labels != -1).all()\n",
    "assert (val_labels != -1).all()\n",
    "\n",
    "print(\"Train labels value counts:\", pd.Series(train_labels).value_counts())\n",
    "print(\"Val labels value counts:\", pd.Series(val_labels).value_counts())\n",
    "\n",
    "# 4. 构造真正给 Trainer 用的 Dataset（带 labels）\n",
    "train_dataset = FinetuneDataset(\n",
    "    base_corpus=all_corpus,\n",
    "    indices=train_idx,\n",
    "    labels_array=train_labels,\n",
    ")\n",
    "val_dataset = FinetuneDataset(\n",
    "    base_corpus=all_corpus,\n",
    "    indices=val_idx,\n",
    "    labels_array=val_labels,\n",
    ")\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Val dataset size:\", len(val_dataset))\n",
    "\n",
    "# 5. Trainer\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "trainer.save_model(args.output)\n",
    "\n",
    "# 保存 label encoder\n",
    "dump(le, open(os.path.join(args.output, \"label_encoder.pkl\"), \"wb\"))\n",
    "print(f\"Model and label encoder saved to: {args.output}\")\n",
    "\n",
    "# 保存日志\n",
    "logs = trainer.state.log_history\n",
    "logs_df = pd.DataFrame(logs)\n",
    "\n",
    "os.makedirs(args.log, exist_ok=True)\n",
    "log_path = os.path.join(args.log, \"finetune_log.csv\")\n",
    "logs_df.to_csv(log_path, index=False)\n",
    "\n",
    "print(f\"Training logs saved to: {log_path}\")\n",
    "logs_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e6606-fe1b-4486-8826-fd61caed76f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (caiqy_MiCoSeq_dev)",
   "language": "python",
   "name": "caiqy_micoseq_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
