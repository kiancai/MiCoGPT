{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d8b9b5",
   "metadata": {},
   "source": [
    "## 0. 导入依赖 (Import Dependencies)\n",
    "\n",
    "**[Ablation Study: Value + Condition]**\n",
    "这是消融实验的进阶版。\n",
    "在此配置中，我们将：\n",
    "1. **保留 Value Embeddings** (丰度信息)\n",
    "2. **保留 Condition Embeddings** (环境元数据)\n",
    "3. **关闭 Cross-Attention** (不使用先验知识库)\n",
    "4. 验证在没有 Cross-Attention 的情况下，仅靠环境上下文能带来多少提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfa508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from pickle import load\n",
    "from argparse import Namespace\n",
    "from configparser import ConfigParser\n",
    "from importlib.resources import files\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "from MiCoGPT.utils_vCross.model_vCross import MiCoGPTConfig, MiCoGPTForCausalLM\n",
    "from MiCoGPT.utils_vCross.collator_vCross import MiCoGPTDataCollator\n",
    "from MiCoGPT.utils.tools import split_train_val_by_project_stratified\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e812d7c",
   "metadata": {},
   "source": [
    "## 1. 基本参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    input=\"../data/vCross/ResMicroDB_90338_vCross.pkl\",\n",
    "    # [Ablation] 输出路径: Value + Condition\n",
    "    output=\"../models/pretrain_vCross_value_condition\",\n",
    "    log=\"../logs/pretrain_vCross_value_condition\",\n",
    "    prior_npz=None\n",
    ")\n",
    "VAL_RATIO = 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f86f6",
   "metadata": {},
   "source": [
    "## 2. 载入语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading corpus from {args.input} ...\")\n",
    "all_corpus = load(open(args.input, \"rb\"))\n",
    "\n",
    "if all_corpus.metadata is not None and \"Split_Group\" in all_corpus.metadata.columns:\n",
    "    print(\"Subsetting corpus by Split_Group == 'A'...\")\n",
    "    corpus = all_corpus.subset_by_metadata(lambda df: df[\"Split_Group\"] == \"A\")\n",
    "else:\n",
    "    print(\"Using full corpus (no Split_Group found or metadata missing).\")\n",
    "    corpus = all_corpus\n",
    "    \n",
    "tokenizer = all_corpus.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meta_info",
   "metadata": {},
   "source": [
    "## 3. 提取环境元数据信息\n",
    "\n",
    "这里我们需要正常提取 `condition_vocab_sizes`，因为我们要使用 Condition Embeddings。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_meta",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(corpus, Subset):\n",
    "    base_corpus = corpus.dataset\n",
    "else:\n",
    "    base_corpus = corpus\n",
    "\n",
    "condition_cols = list(base_corpus.meta_encoders.keys())\n",
    "condition_vocab_sizes = [len(le.classes_) + 1 for le in base_corpus.meta_encoders.values()]\n",
    "\n",
    "print(\"Condition Columns:\", condition_cols)\n",
    "print(\"Condition Vocab Sizes:\", condition_vocab_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2283855",
   "metadata": {},
   "source": [
    "## 4. 构建模型 (Value + Condition)\n",
    "\n",
    "配置：\n",
    "1. `num_bins=52` (保留 Value)\n",
    "2. `condition_vocab_sizes=[...]` (保留 Condition)\n",
    "3. `prior_matrix_path=None` & `add_cross_attention=False` (关闭 Cross-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ConfigParser()\n",
    "cfg.read(files(\"MiCoGPT\")/\"resources/config.ini\")\n",
    "\n",
    "gpt2_config_dict = {\n",
    "    \"vocab_size\":   tokenizer.vocab_size,\n",
    "    \"n_positions\":  cfg.getint(\"GPT2\", \"n_positions\"),\n",
    "    \"n_embd\":       cfg.getint(\"GPT2\", \"n_embd\"),\n",
    "    \"n_layer\":      cfg.getint(\"GPT2\", \"n_layer\"),\n",
    "    \"n_head\":       cfg.getint(\"GPT2\", \"n_head\"),\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "\n",
    "config = MiCoGPTConfig(\n",
    "    num_bins=52,                        # 保留 Value\n",
    "    condition_vocab_sizes=condition_vocab_sizes, # 保留 Condition\n",
    "    prior_matrix_path=None,             # 不用 Prior\n",
    "    add_cross_attention=False,          # 关闭 Cross-Attention\n",
    "    **gpt2_config_dict\n",
    ")\n",
    "\n",
    "model = MiCoGPTForCausalLM(config)\n",
    "print(\"Model Config:\", config)\n",
    "print(\"Model Architecture:\", model)\n",
    "print(\"\\n[Check] Cross Attention is:\", \"ENABLED\" if config.add_cross_attention else \"DISABLED\")\n",
    "print(\"[Check] Condition Embeddings:\", len(model.condition_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collator",
   "metadata": {},
   "source": [
    "## 5. 初始化数据整理器 (正常模式)\n",
    "\n",
    "使用标准的 `MiCoGPTDataCollator` 即可，它会正常返回 `input_ids`, `value_ids`, `condition_ids`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_collator",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = MiCoGPTDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.n_positions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(corpus, Subset):\n",
    "    metadata = corpus.dataset.metadata\n",
    "else:\n",
    "    metadata = corpus.metadata\n",
    "\n",
    "if metadata is not None and \"Project_ID\" in metadata.columns:\n",
    "    print(\"Using stratified split by Project_ID...\")\n",
    "    train_dataset, val_dataset = split_train_val_by_project_stratified(\n",
    "        corpus,\n",
    "        val_ratio=VAL_RATIO,\n",
    "        project_col=\"Project_ID\"\n",
    "    )\n",
    "else:\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        corpus, \n",
    "        [len(corpus)-int(len(corpus)*VAL_RATIO), int(len(corpus)*VAL_RATIO)], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_args_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{args.output}/checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=args.log,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    report_to=[\"tensorboard\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trainer.save_model(args.output)\n",
    "tokenizer.save_pretrained(args.output)\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "with open(f\"{args.output}/training_logs.json\", \"w\") as f:\n",
    "    json.dump(log_history, f, indent=2)\n",
    "\n",
    "train_steps = [x[\"step\"] for x in log_history if \"loss\" in x]\n",
    "train_loss = [x[\"loss\"] for x in log_history if \"loss\" in x]\n",
    "eval_steps = [x[\"step\"] for x in log_history if \"eval_loss\" in x]\n",
    "eval_loss = [x[\"eval_loss\"] for x in log_history if \"eval_loss\" in x]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "if train_steps: plt.plot(train_steps, train_loss, label=\"Training Loss\", alpha=0.7)\n",
    "if eval_steps: plt.plot(eval_steps, eval_loss, label=\"Validation Loss\", marker=\"o\", linestyle=\"--\")\n",
    "plt.title(\"Ablation: Value + Condition (No Cross-Attn)\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{args.output}/loss_curve.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
