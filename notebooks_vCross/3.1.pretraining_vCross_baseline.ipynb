{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d8b9b5",
   "metadata": {},
   "source": [
    "## 0. 导入依赖 (Import Dependencies)\n",
    "\n",
    "**[Ablation Study: Baseline (Only Species)]**\n",
    "这是消融实验的基准线 (Baseline)。\n",
    "在此配置中，我们将模型退化为标准的 GPT-2：\n",
    "1. **关闭 Value Embeddings** (设 num_bins=0，或者通过不传 value_ids 实现)\n",
    "2. **关闭 Condition Embeddings**\n",
    "3. **关闭 Cross-Attention**\n",
    "4. 仅使用 **Species Embeddings** (物种 ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfa508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from pickle import load\n",
    "from argparse import Namespace\n",
    "from configparser import ConfigParser\n",
    "from importlib.resources import files\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "from MiCoGPT.utils_vCross.model_vCross import MiCoGPTConfig, MiCoGPTForCausalLM\n",
    "from MiCoGPT.utils_vCross.collator_vCross import MiCoGPTDataCollator\n",
    "from MiCoGPT.utils.tools import split_train_val_by_project_stratified\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e812d7c",
   "metadata": {},
   "source": [
    "## 1. 基本参数设置 (Basic Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    input=\"../data/vCross/ResMicroDB_90338_vCross.pkl\",\n",
    "    # [Ablation] 输出路径: Baseline\n",
    "    output=\"../models/pretrain_vCross_baseline\",\n",
    "    log=\"../logs/pretrain_vCross_baseline\",\n",
    "    prior_npz=None\n",
    ")\n",
    "VAL_RATIO = 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f86f6",
   "metadata": {},
   "source": [
    "## 2. 载入语料库 (Load Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading corpus from {args.input} ...\")\n",
    "all_corpus = load(open(args.input, \"rb\"))\n",
    "\n",
    "if all_corpus.metadata is not None and \"Split_Group\" in all_corpus.metadata.columns:\n",
    "    print(\"Subsetting corpus by Split_Group == 'A'...\")\n",
    "    corpus = all_corpus.subset_by_metadata(lambda df: df[\"Split_Group\"] == \"A\")\n",
    "else:\n",
    "    print(\"Using full corpus (no Split_Group found or metadata missing).\")\n",
    "    corpus = all_corpus\n",
    "    \n",
    "tokenizer = all_corpus.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2283855",
   "metadata": {},
   "source": [
    "## 3. 构建 Baseline 模型 (Only Species)\n",
    "\n",
    "为了禁用 Value Embedding，我们将 `num_bins` 设为 0 (或者在 collator 中不传 value_ids)。\n",
    "由于我们的代码里 `value_ids` 默认会加上去，最简单的办法是保留 embedding 层但不给它传入有效数据，\n",
    "或者更彻底一点：**在 Config 里设置 `num_bins=0` 并修改 collator 传全0**。\n",
    "\n",
    "这里为了不动模型代码，我们**在 Config 中保留 num_bins=52**，但**修改 Collator 逻辑**，让其不返回 `value_ids` (或者全返回 0)。\n",
    "不过，最干净的方法是利用 PyTorch 的特性：如果 `value_ids` 是 None，我们的模型代码会跳过加法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ConfigParser()\n",
    "cfg.read(files(\"MiCoGPT\")/\"resources/config.ini\")\n",
    "\n",
    "gpt2_config_dict = {\n",
    "    \"vocab_size\":   tokenizer.vocab_size,\n",
    "    \"n_positions\":  cfg.getint(\"GPT2\", \"n_positions\"),\n",
    "    \"n_embd\":       cfg.getint(\"GPT2\", \"n_embd\"),\n",
    "    \"n_layer\":      cfg.getint(\"GPT2\", \"n_layer\"),\n",
    "    \"n_head\":       cfg.getint(\"GPT2\", \"n_head\"),\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "\n",
    "config = MiCoGPTConfig(\n",
    "    num_bins=52,          # 保持结构，但在输入端屏蔽\n",
    "    condition_vocab_sizes=[], \n",
    "    prior_matrix_path=None,\n",
    "    add_cross_attention=False,\n",
    "    **gpt2_config_dict\n",
    ")\n",
    "\n",
    "model = MiCoGPTForCausalLM(config)\n",
    "print(\"Baseline Model Built (Only Species)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collator",
   "metadata": {},
   "source": [
    "## 4. 初始化数据整理器 (屏蔽 Value)\n",
    "\n",
    "我们需要继承 `MiCoGPTDataCollator` 并覆盖 `__call__`，或者简单地在实例化后给它打个补丁。\n",
    "这里我们定义一个 `BaselineDataCollator`，它继承原版，但在返回前把 `value_ids` 设为 None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_collator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineDataCollator(MiCoGPTDataCollator):\n",
    "    def __call__(self, examples):\n",
    "        batch = super().__call__(examples)\n",
    "        # [Baseline Key] 屏蔽 Value Embedding\n",
    "        # 将 value_ids 设为 None，模型 forward 时就会跳过 value embedding 的加法\n",
    "        batch[\"value_ids\"] = None \n",
    "        return batch\n",
    "\n",
    "collator = BaselineDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.n_positions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_data",
   "metadata": {},
   "source": [
    "## 5. 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(corpus, Subset):\n",
    "    metadata = corpus.dataset.metadata\n",
    "else:\n",
    "    metadata = corpus.metadata\n",
    "\n",
    "if metadata is not None and \"Project_ID\" in metadata.columns:\n",
    "    print(\"Using stratified split by Project_ID...\")\n",
    "    train_dataset, val_dataset = split_train_val_by_project_stratified(\n",
    "        corpus,\n",
    "        val_ratio=VAL_RATIO,\n",
    "        project_col=\"Project_ID\"\n",
    "    )\n",
    "else:\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        corpus, \n",
    "        [len(corpus)-int(len(corpus)*VAL_RATIO), int(len(corpus)*VAL_RATIO)], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_args_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{args.output}/checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=args.log,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    report_to=[\"tensorboard\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trainer.save_model(args.output)\n",
    "tokenizer.save_pretrained(args.output)\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "with open(f\"{args.output}/training_logs.json\", \"w\") as f:\n",
    "    json.dump(log_history, f, indent=2)\n",
    "\n",
    "train_steps = [x[\"step\"] for x in log_history if \"loss\" in x]\n",
    "train_loss = [x[\"loss\"] for x in log_history if \"loss\" in x]\n",
    "eval_steps = [x[\"step\"] for x in log_history if \"eval_loss\" in x]\n",
    "eval_loss = [x[\"eval_loss\"] for x in log_history if \"eval_loss\" in x]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "if train_steps: plt.plot(train_steps, train_loss, label=\"Training Loss\", alpha=0.7)\n",
    "if eval_steps: plt.plot(eval_steps, eval_loss, label=\"Validation Loss\", marker=\"o\", linestyle=\"--\")\n",
    "plt.title(\"Ablation: Baseline (Only Species)\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{args.output}/loss_curve.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
