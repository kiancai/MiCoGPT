{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d8b9b5",
   "metadata": {},
   "source": [
    "## 0. 导入依赖 (Import Dependencies)\n",
    "\n",
    "**[Ablation Study: Only Value]**\n",
    "这是一个消融实验 Notebook，旨在验证“仅使用数值信息 (Abundance Value)”对模型性能的影响。\n",
    "在此配置中，我们将：\n",
    "1. **关闭 Cross-Attention** (不使用先验知识库)\n",
    "2. **关闭 Condition Embeddings** (不使用环境元数据)\n",
    "3. 仅保留 **Species Embeddings** (物种 ID) + **Value Embeddings** (丰度等级)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfa508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from pickle import load\n",
    "from argparse import Namespace\n",
    "from configparser import ConfigParser\n",
    "from importlib.resources import files\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "# --- v2.0 新增模块 ---\n",
    "from MiCoGPT.utils_vCross.model_vCross import MiCoGPTConfig, MiCoGPTForCausalLM\n",
    "from MiCoGPT.utils_vCross.collator_vCross import MiCoGPTDataCollator\n",
    "\n",
    "# 复用 v4 的工具函数\n",
    "from MiCoGPT.utils.tools import split_train_val_by_project_stratified\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e812d7c",
   "metadata": {},
   "source": [
    "## 1. 基本参数设置 (Basic Parameters)\n",
    "\n",
    "注意：这里 `output` 和 `log` 路径已修改为 `_onlyValue` 后缀，以免覆盖主模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # 语料库文件路径\n",
    "    input=\"../data/vCross/ResMicroDB_90338_vCross.pkl\",\n",
    "    \n",
    "    # [Ablation] 修改输出路径，区分实验\n",
    "    output=\"../models/pretrain_vCross_onlyValue\",\n",
    "    log=\"../logs/pretrain_vCross_onlyValue\",\n",
    "    \n",
    "    # [Ablation] 不需要 prior_npz，设为 None\n",
    "    prior_npz=None\n",
    ")\n",
    "\n",
    "# 验证集比例\n",
    "VAL_RATIO = 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f86f6",
   "metadata": {},
   "source": [
    "## 2. 载入语料库 (Load Corpus)\n",
    "\n",
    "载入数据，并提取必要的维度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading corpus from {args.input} ...\")\n",
    "all_corpus = load(open(args.input, \"rb\"))\n",
    "\n",
    "# 检查是否包含了 metadata 和 encoders\n",
    "if not hasattr(all_corpus, \"meta_encoders\") or not all_corpus.meta_encoders:\n",
    "    print(\"[Warning] Corpus object does not contain meta_encoders. Please check if it was constructed correctly.\")\n",
    "\n",
    "# 选择 Split_Group 为 A 的样本进行训练\n",
    "if all_corpus.metadata is not None and \"Split_Group\" in all_corpus.metadata.columns:\n",
    "    print(\"Subsetting corpus by Split_Group == 'A'...\")\n",
    "    corpus = all_corpus.subset_by_metadata(lambda df: df[\"Split_Group\"] == \"A\")\n",
    "else:\n",
    "    print(\"Using full corpus (no Split_Group found or metadata missing).\")\n",
    "    corpus = all_corpus\n",
    "    \n",
    "tokenizer = all_corpus.tokenizer\n",
    "\n",
    "print(\"Number of samples in all_corpus:\", len(all_corpus))\n",
    "print(\"Number of samples in corpus (Training subset):\", len(corpus))\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2283855",
   "metadata": {},
   "source": [
    "## 3. 构建消融模型 (Build Ablation Model)\n",
    "\n",
    "关键修改：\n",
    "1. `add_cross_attention=False`: 关闭先验知识注入。\n",
    "2. `condition_vocab_sizes=[]`: 告诉模型没有环境元数据列。\n",
    "3. `prior_matrix_path=None`: 不加载先验矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础配置读取\n",
    "cfg = ConfigParser()\n",
    "cfg.read(files(\"MiCoGPT\")/\"resources/config.ini\")\n",
    "\n",
    "# 1. 构建配置\n",
    "gpt2_config_dict = {\n",
    "    \"vocab_size\":   tokenizer.vocab_size,\n",
    "    \"n_positions\":  cfg.getint(\"GPT2\", \"n_positions\"),\n",
    "    \"n_embd\":       cfg.getint(\"GPT2\", \"n_embd\"),\n",
    "    \"n_layer\":      cfg.getint(\"GPT2\", \"n_layer\"),\n",
    "    \"n_head\":       cfg.getint(\"GPT2\", \"n_head\"),\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "\n",
    "config = MiCoGPTConfig(\n",
    "    num_bins=52,                        # 保留 Value Embedding\n",
    "    \n",
    "    # [Ablation] 关闭 Condition Embeddings\n",
    "    condition_vocab_sizes=[], \n",
    "    \n",
    "    # [Ablation] 不使用 Prior Matrix\n",
    "    prior_matrix_path=None,\n",
    "    \n",
    "    # [Ablation] 显式关闭 Cross-Attention\n",
    "    add_cross_attention=False,\n",
    "    \n",
    "    **gpt2_config_dict\n",
    ")\n",
    "\n",
    "# 2. 实例化模型\n",
    "model = MiCoGPTForCausalLM(config)\n",
    "\n",
    "print(\"Model Config:\", config)\n",
    "print(\"Model Architecture:\", model)\n",
    "print(\"\\n[Check] Cross Attention is:\", \"ENABLED\" if config.add_cross_attention else \"DISABLED\")\n",
    "print(\"[Check] Condition Embeddings:\", len(model.condition_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collator",
   "metadata": {},
   "source": [
    "## 4. 初始化数据整理器 (Data Collator)\n",
    "\n",
    "注意：虽然我们关闭了 Condition Embeddings，但 Collator 还是会生成 `condition_ids`。\n",
    "不过没关系，只要 Config 里设了 `condition_vocab_sizes=[]`，模型就不会去使用这些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_collator",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = MiCoGPTDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.n_positions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_data",
   "metadata": {},
   "source": [
    "## 5. 划分训练集和验证集 (Split Train/Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 metadata (兼容 Subset)\n",
    "if isinstance(corpus, Subset):\n",
    "    metadata = corpus.dataset.metadata\n",
    "else:\n",
    "    metadata = corpus.metadata\n",
    "\n",
    "# 检查是否可以按 Project_ID 分层\n",
    "if metadata is not None and \"Project_ID\" in metadata.columns:\n",
    "    print(\"Using stratified split by Project_ID...\")\n",
    "    train_dataset, val_dataset = split_train_val_by_project_stratified(\n",
    "        corpus,\n",
    "        val_ratio=VAL_RATIO,\n",
    "        project_col=\"Project_ID\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Project_ID not found or metadata missing. Using random split...\")\n",
    "    val_size = int(len(corpus) * VAL_RATIO)\n",
    "    train_size = len(corpus) - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        corpus, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_args",
   "metadata": {},
   "source": [
    "## 6. 训练参数 (Training Arguments)\n",
    "\n",
    "与主实验保持一致，除了 output_dir。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_args_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{args.output}/checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    \n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    logging_dir=args.log,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # [NFS Fix] \n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    \n",
    "    report_to=[\"tensorboard\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainer_init",
   "metadata": {},
   "source": [
    "## 7. 开始训练 (Start Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_model",
   "metadata": {},
   "source": [
    "## 8. 保存模型与日志 (Save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 保存模型\n",
    "trainer.save_model(args.output)\n",
    "tokenizer.save_pretrained(args.output)\n",
    "print(f\"Model saved to {args.output}\")\n",
    "\n",
    "# 2. 导出日志\n",
    "log_history = trainer.state.log_history\n",
    "log_path = f\"{args.output}/training_logs.json\"\n",
    "with open(log_path, \"w\") as f:\n",
    "    json.dump(log_history, f, indent=2)\n",
    "print(f\"Training logs saved to {log_path}\")\n",
    "\n",
    "# 3. 绘制曲线\n",
    "train_steps = [x[\"step\"] for x in log_history if \"loss\" in x]\n",
    "train_loss = [x[\"loss\"] for x in log_history if \"loss\" in x]\n",
    "eval_steps = [x[\"step\"] for x in log_history if \"eval_loss\" in x]\n",
    "eval_loss = [x[\"eval_loss\"] for x in log_history if \"eval_loss\" in x]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "if train_steps:\n",
    "    plt.plot(train_steps, train_loss, label=\"Training Loss\", alpha=0.7)\n",
    "if eval_steps:\n",
    "    plt.plot(eval_steps, eval_loss, label=\"Validation Loss\", marker=\"o\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Ablation Study: Only Value (No Cross-Attn, No Condition)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.savefig(f\"{args.output}/loss_curve.png\")\n",
    "print(f\"Loss curve saved to {args.output}/loss_curve.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
