{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# MiCoGPT v2.0 Fine-tuning (Sequence Classification)\n",
    "\n",
    "本 Notebook 演示如何加载预训练好的 `MiCoGPT v2.0` 模型 (vCross 版)，并在下游任务 (例如疾病预测/二分类) 上进行微调。\n",
    "\n",
    "**主要流程：**\n",
    "1. 加载 `vCross` 版本的多模态语料库。\n",
    "2. 筛选目标子集 (例如 Split_Group=A) 并准备分类标签。\n",
    "3. 加载预训练模型，并转换为分类模型 (`MiCoGPTForSequenceClassification`)。\n",
    "4. 使用自定义的 `MiCoGPTClassificationCollator` 处理多模态输入和分类标签。\n",
    "5. 训练并评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pickle import load as pkl_load\n",
    "from argparse import Namespace\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "# --- v2.0 自定义模块 ---\n",
    "from MiCoGPT.utils_vCross.model_vCross import MiCoGPTConfig, MiCoGPTForSequenceClassification\n",
    "from MiCoGPT.utils_vCross.collator_vCross import MiCoGPTClassificationCollator\n",
    "\n",
    "# --- 复用工具函数 ---\n",
    "from MiCoGPT.utils.finetune import prepare_labels_for_subset, split_train_val_by_project_stratified_with_labels\n",
    "from MiCoGPT.utils.finetune_v2 import SubsetWithLabels\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## 1. 配置参数 (Configuration)\n",
    "\n",
    "设置输入数据路径、预训练模型路径和微调参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "args",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # 输入语料库 (vCross 格式)\n",
    "    input=\"../data/vCross/ResMicroDB_90338_vCross.pkl\",\n",
    "    \n",
    "    # 预训练模型路径 (从 checkpoint 加载)\n",
    "    pretrained_model=\"../models/pretrain_vCross_base\",\n",
    "    \n",
    "    # 输出目录\n",
    "    output=\"../models/finetuned_vCross_base\",\n",
    "    log=\"../logs/finetuned_vCross_base\",\n",
    "    \n",
    "    # 任务相关\n",
    "    label_col=\"Is_Healthy\",     # 预测标签列\n",
    "    split_group=\"A\",            # 仅使用 Split_Group A 进行微调\n",
    "    val_ratio=0.2,              # 验证集比例\n",
    ")\n",
    "\n",
    "print(\"Args:\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_corpus",
   "metadata": {},
   "source": [
    "## 2. 加载语料库 (Load Corpus)\n",
    "\n",
    "读取 `MiCoGPTCorpus_vCross` 对象，包含多模态数据 (Token IDs, Value IDs, Condition IDs) 和元数据 (Metadata)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_pkl",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading corpus from {args.input} ...\")\n",
    "with open(args.input, \"rb\") as f:\n",
    "    corpus = pkl_load(f)\n",
    "    \n",
    "print(f\"Loaded corpus with {len(corpus)} samples.\")\n",
    "print(\"Tokenizer vocab size:\", corpus.tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare_data",
   "metadata": {},
   "source": [
    "## 3. 数据准备 (Data Preparation)\n",
    "\n",
    "1. **筛选子集**: 选择 `Split_Group == A` 且标签存在的样本。\n",
    "2. **生成标签**: 将文本标签 (如 True/False) 转换为数字 ID。\n",
    "3. **划分数据集**: 按 Project 对应的 Study 进行分层划分，防止数据泄漏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter_subset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 筛选子集\n",
    "meta = corpus.metadata\n",
    "valid_mask = (meta[\"Split_Group\"] == args.split_group) & (meta[args.label_col].notna())\n",
    "finetune_indices = np.where(valid_mask)[0]\n",
    "finetune_subset = Subset(corpus, finetune_indices)\n",
    "\n",
    "print(f\"Filtered subset size: {len(finetune_subset)}\")\n",
    "\n",
    "# 2. 生成标签 (prepare_labels_for_subset 来自 MiCoGPT.utils.finetune)\n",
    "# 它会返回一个与 corpus 长度对齐的 all_labels 数组 (非 subset 位置为 -1)\n",
    "labels_tensor, all_labels_array, le, num_labels = prepare_labels_for_subset(\n",
    "    all_corpus=corpus,\n",
    "    subset=finetune_subset,\n",
    "    label_col=args.label_col,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3. 划分训练/验证集\n",
    "train_subset, val_subset = split_train_val_by_project_stratified_with_labels(\n",
    "    finetune_subset,\n",
    "    label_col=args.label_col,\n",
    "    val_ratio=args.val_ratio,\n",
    "    project_col=\"Project_ID\"\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_subset)}\")\n",
    "print(f\"Val size: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrap_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 包装成带 Labels 的 Dataset\n",
    "def create_dataset_with_labels(subset, all_labels_array):\n",
    "    # subset.indices 是在 corpus 中的全局索引\n",
    "    indices = subset.indices\n",
    "    # 从全局 labels 数组中提取对应标签\n",
    "    subset_labels = torch.tensor(all_labels_array[indices], dtype=torch.long)\n",
    "    return SubsetWithLabels(subset, subset_labels)\n",
    "\n",
    "train_ds = create_dataset_with_labels(train_subset, all_labels_array)\n",
    "val_ds = create_dataset_with_labels(val_subset, all_labels_array)\n",
    "\n",
    "# 检查一个样本\n",
    "sample = train_ds[0]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"Label:\", sample[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_model",
   "metadata": {},
   "source": [
    "## 4. 加载模型 (Load Model)\n",
    "\n",
    "加载预训练的 `vCross` 模型，并实例化为 `MiCoGPTForSequenceClassification`。\n",
    "注意：\n",
    "- 我们需要指定 `num_labels`。\n",
    "- `ignore_mismatched_sizes=True` 是必须的，因为分类头 (Score Layer) 是新初始化的，与预训练的 LM Head 尺寸不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载配置\n",
    "config = MiCoGPTConfig.from_pretrained(args.pretrained_model)\n",
    "config.num_labels = num_labels\n",
    "print(\"Model Config:\", config)\n",
    "\n",
    "# 加载模型\n",
    "model = MiCoGPTForSequenceClassification.from_pretrained(\n",
    "    args.pretrained_model,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# 打印模型结构，确认多模态 Embedding 和分类头存在\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training",
   "metadata": {},
   "source": [
    "## 5. 训练 (Training)\n",
    "\n",
    "使用 HuggingFace Trainer 进行微调。\n",
    "- 使用 `MiCoGPTClassificationCollator` 处理数据。\n",
    "- 定义评估指标 (Accuracy, F1)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=args.output,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    \n",
    "    logging_dir=args.log,\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # NFS 优化\n",
    "    dataloader_num_workers=0,\n",
    "    \n",
    "    # 显存优化 (防止 Evaluation OOM)\n",
    "    # eval_accumulation_steps=60,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # 使用自定义的分类 Collator\n",
    "    data_collator=MiCoGPTClassificationCollator(corpus.tokenizer, max_length=512),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save",
   "metadata": {},
   "source": [
    "## 6. 保存与分析 (Save & Analyze)\n",
    "\n",
    "保存微调后的模型、Tokenizer 以及训练日志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 保存模型\n",
    "trainer.save_model(args.output)\n",
    "corpus.tokenizer.save_pretrained(args.output)\n",
    "\n",
    "# 保存 Label Encoder (关键)\n",
    "import joblib\n",
    "joblib.dump(le, f\"{args.output}/label_encoder.joblib\")\n",
    "print(f\"Model and LabelEncoder saved to {args.output}\")\n",
    "\n",
    "# 2. 导出日志\n",
    "log_history = trainer.state.log_history\n",
    "with open(f\"{args.output}/training_logs.json\", \"w\") as f:\n",
    "    json.dump(log_history, f, indent=2)\n",
    "\n",
    "# 3. 绘制曲线\n",
    "train_steps = [x[\"step\"] for x in log_history if \"loss\" in x]\n",
    "train_loss = [x[\"loss\"] for x in log_history if \"loss\" in x]\n",
    "eval_steps = [x[\"step\"] for x in log_history if \"eval_loss\" in x]\n",
    "eval_loss = [x[\"eval_loss\"] for x in log_history if \"eval_loss\" in x]\n",
    "eval_f1 = [x[\"eval_f1\"] for x in log_history if \"eval_f1\" in x]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss 曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "if train_steps: plt.plot(train_steps, train_loss, label=\"Train Loss\", alpha=0.6)\n",
    "if eval_steps: plt.plot(eval_steps, eval_loss, label=\"Val Loss\", marker=\"o\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "\n",
    "# F1 曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "if eval_steps and eval_f1: \n",
    "    plt.plot(eval_steps, eval_f1, label=\"Val F1\", color=\"orange\", marker=\"s\")\n",
    "plt.title(\"F1 Score Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{args.output}/training_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reload_header",
   "metadata": {},
   "source": [
    "## 7.5. (可选) 重载模型 (Reload Model)\n",
    "\n",
    "如果内存已清空，可以运行此 Cell 重载模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reload_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. (可选) 重新加载模型和配置\n",
    "\n",
    "import joblib\n",
    "from MiCoGPT.utils_vCross.model_vCross import MiCoGPTConfig, MiCoGPTForSequenceClassification\n",
    "\n",
    "# 检查变量是否存在，如果不存在则加载\n",
    "if 'trainer' not in locals() or 'model' not in locals() or 'le' not in locals():\n",
    "    print(f\"Loading model from {args.output} ...\")\n",
    "    \n",
    "    # 1. 加载 Label Encoder\n",
    "    le_path = f\"{args.output}/label_encoder.joblib\"\n",
    "    if os.path.exists(le_path):\n",
    "        le = joblib.load(le_path)\n",
    "        print(\"Label Encoder loaded.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Label Encoder not found at {le_path}. Please run training first.\")\n",
    "    \n",
    "    # 2. 加载模型\n",
    "    if hasattr(le, \"classes_\"):\n",
    "        num_labels = len(le.classes_)\n",
    "    else:\n",
    "        num_labels = len(le.categories_[0])\n",
    "        \n",
    "    config = MiCoGPTConfig.from_pretrained(args.output)\n",
    "    model = MiCoGPTForSequenceClassification.from_pretrained(args.output, config=config)\n",
    "    \n",
    "    # 3. 准备 Collator\n",
    "    if 'MiCoGPTClassificationCollator' not in locals():\n",
    "        from MiCoGPT.utils_vCross.collator_vCross import MiCoGPTClassificationCollator\n",
    "        \n",
    "    if 'corpus' in locals():\n",
    "        tokenizer = corpus.tokenizer\n",
    "    else:\n",
    "        print(\"Warning: corpus not in memory. Using tokenizer from saved model.\")\n",
    "        raise ValueError(\"Please run the 'Load Corpus' cell above to load the tokenizer first.\")\n",
    "        \n",
    "    data_collator = MiCoGPTClassificationCollator(tokenizer)\n",
    "    \n",
    "    # 4. 重建 Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(output_dir=args.output, per_device_eval_batch_size=32),\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    print(\"Model and Trainer restored.\")\n",
    "else:\n",
    "    print(\"Model and Trainer already in memory. Skipping reload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction",
   "metadata": {},
   "source": [
    "## 8. 预测 (Prediction on Split B)\n",
    "\n",
    "使用微调后的模型对测试集 (Split_Group='B') 进行预测，并调用 `eval_and_save` 计算多项指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MiCoGPT.utils.mgm_utils import eval_and_save\n",
    "\n",
    "# 1. 准备测试集 (Split_Group = B)\n",
    "print(\"Preparing Test Set (Split_Group='B')...\")\n",
    "meta = corpus.metadata\n",
    "test_mask = (meta[\"Split_Group\"] == \"B\") & (meta[args.label_col].notna())\n",
    "test_indices = np.where(test_mask)[0]\n",
    "test_subset = Subset(corpus, test_indices)\n",
    "\n",
    "print(f\"Test subset size: {len(test_subset)}\")\n",
    "\n",
    "# 2. 生成测试集标签 (复用训练时的 Encoder)\n",
    "# 注意：这里必须传入训练时 fit 好的 le (encoder)，以保证标签 ID 映射一致\n",
    "test_labels_tensor, test_all_labels, _, _ = prepare_labels_for_subset(\n",
    "    all_corpus=corpus,\n",
    "    subset=test_subset,\n",
    "    label_col=args.label_col,\n",
    "    encoder=le, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3. 包装 Dataset\n",
    "test_ds = create_dataset_with_labels(test_subset, test_all_labels)\n",
    "\n",
    "# 4. 预测\n",
    "print(\"Running prediction...\")\n",
    "# Trainer 会自动使用最佳模型 (load_best_model_at_end=True)\n",
    "predictions = trainer.predict(test_ds)\n",
    "y_score = predictions.predictions\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# 5. 评估并保存\n",
    "# 获取类别名称\n",
    "if hasattr(le, \"categories_\"):\n",
    "    # OneHotEncoder\n",
    "    label_names = list(le.categories_[0])\n",
    "else:\n",
    "    # LabelEncoder\n",
    "    label_names = [str(c) for c in le.classes_]\n",
    "\n",
    "save_dir = f\"{args.output}/prediction_B\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving results to {save_dir}...\")\n",
    "eval_and_save(\n",
    "    y_score=y_score,\n",
    "    y_true=y_true,\n",
    "    label_names=label_names,\n",
    "    save_dir=save_dir,\n",
    "    activation=\"softmax\" # 模型输出 logits，需要 softmax 归一化\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
