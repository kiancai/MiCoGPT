{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查生成的 vCross Corpus 结构\n",
    "本 Notebook 用于加载并验证生成的 `ResMicroDB_90338_vCross.pkl` 文件，确保所有数据结构（Input IDs, Value IDs, Condition IDs）均正确实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from ../data/vCorss/ResMicroDB_90338_vCross.pkl...\n",
      "Corpus loaded successfully. Total samples: 90338\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from MiCoGPT.utils_vCross.corpus_vCross import MiCoGPTCorpusVCross\n",
    "\n",
    "# 路径设置 (请根据实际情况修改)\n",
    "corpus_path = \"../data/vCorss/ResMicroDB_90338_vCross.pkl\"\n",
    "encoder_path = \"../data/vCorss/meta_encoders.joblib\"\n",
    "\n",
    "print(f\"Loading corpus from {corpus_path}...\")\n",
    "with open(corpus_path, \"rb\") as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "print(f\"Corpus loaded successfully. Total samples: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基础属性检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes check:\n",
      "- num_bins: 51\n",
      "- log1p: True\n",
      "- normalize_total: None (If None, means adaptive)\n",
      "- max_len: 512\n",
      "- use_meta_cols: ['Sample_Site']\n",
      "- Metadata Encoders keys: ['Sample_Site']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Attributes check:\")\n",
    "print(f\"- num_bins: {corpus.num_bins}\")\n",
    "print(f\"- log1p: {corpus.log1p}\")\n",
    "print(f\"- normalize_total: {corpus.normalize_total} (If None, means adaptive)\")\n",
    "print(f\"- max_len: {corpus.max_len}\")\n",
    "print(f\"- use_meta_cols: {corpus.use_meta_cols}\")\n",
    "print(f\"- Metadata Encoders keys: {list(corpus.meta_encoders.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 样本数据结构深度检查\n",
    "我们将检查以下关键点：\n",
    "1. **BOS/EOS 完整性**: 每个样本必须以 `<bos>` 开头，以 `<eos>` 结尾（除非被截断）。\n",
    "2. **多模态对齐**: Input IDs (Taxon), Value IDs (Bin), Condition IDs (Meta) 必须严格对应。\n",
    "3. **典型样本展示**: 选取最短、最长、以及前三个样本进行详细 Token 级展示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01e26d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== First Sample (Index: 0, ID: CRR768228) ====================\n",
      "Total Length (with pad): 512\n",
      "Actual Length (tokens):  132\n",
      "Structure Check:\n",
      "  - Starts with BOS? True (ID: 2)\n",
      "  - Ends with EOS?   True (ID: 3, Found: 3)\n",
      "Token Details (Taxon ID | Bin ID):\n",
      "  --- Start ---\n",
      "  [  0] <bos>                (ID:     2) | Bin:   0\n",
      "  [  1] g__Staphylococcus    (ID:   359) | Bin:  51\n",
      "  [  2] g__Mycoplasma        (ID:   476) | Bin:  50\n",
      "  [  3] g__Bacteroides       (ID:   370) | Bin:  50\n",
      "  [  4] g__Corynebacterium   (ID:   374) | Bin:  49\n",
      "  ... ...\n",
      "  --- End ---\n",
      "  [127] g__Flavobacterium    (ID:   507) | Bin:   5\n",
      "  [128] g__TM7a              (ID:   472) | Bin:   9\n",
      "  [129] g__Candidatus_Kaiserbacteria (ID:   450) | Bin:   6\n",
      "  [130] g__Vibrio            (ID:   590) | Bin:   7\n",
      "  [131] <eos>                (ID:     3) | Bin:   0\n",
      "Metadata Conditions:\n",
      "  - Sample_Site: Code=0 -> 'BALF' (Org: 'BALF') ✅\n",
      "\n",
      "==================== Second Sample (Index: 1, ID: CRR768229) ====================\n",
      "Total Length (with pad): 512\n",
      "Actual Length (tokens):  144\n",
      "Structure Check:\n",
      "  - Starts with BOS? True (ID: 2)\n",
      "  - Ends with EOS?   True (ID: 3, Found: 3)\n",
      "Token Details (Taxon ID | Bin ID):\n",
      "  --- Start ---\n",
      "  [  0] <bos>                (ID:     2) | Bin:   0\n",
      "  [  1] g__Stenotrophomonas  (ID:     4) | Bin:  51\n",
      "  [  2] g__Mycoplasma        (ID:   476) | Bin:  50\n",
      "  [  3] g__Bacteroides       (ID:   370) | Bin:  50\n",
      "  [  4] g__Staphylococcus    (ID:   359) | Bin:  49\n",
      "  ... ...\n",
      "  --- End ---\n",
      "  [139] g__Serratia          (ID:   482) | Bin:   2\n",
      "  [140] g__Aerococcus        (ID:   207) | Bin:   2\n",
      "  [141] g__Subgroup_22       (ID:   547) | Bin:   9\n",
      "  [142] g__Aureimonas        (ID:   657) | Bin:  10\n",
      "  [143] <eos>                (ID:     3) | Bin:   0\n",
      "Metadata Conditions:\n",
      "  - Sample_Site: Code=0 -> 'BALF' (Org: 'BALF') ✅\n",
      "\n",
      "==================== Third Sample (Index: 2, ID: CRR768230) ====================\n",
      "Total Length (with pad): 512\n",
      "Actual Length (tokens):  195\n",
      "Structure Check:\n",
      "  - Starts with BOS? True (ID: 2)\n",
      "  - Ends with EOS?   True (ID: 3, Found: 3)\n",
      "Token Details (Taxon ID | Bin ID):\n",
      "  --- Start ---\n",
      "  [  0] <bos>                (ID:     2) | Bin:   0\n",
      "  [  1] g__Ralstonia         (ID:   344) | Bin:  51\n",
      "  [  2] g__Sediminibacterium (ID:   788) | Bin:  50\n",
      "  [  3] g__Enterococcus      (ID:   484) | Bin:  50\n",
      "  [  4] g__Acinetobacter     (ID:   748) | Bin:  50\n",
      "  ... ...\n",
      "  --- End ---\n",
      "  [190] g__R7C24             (ID:   373) | Bin:   5\n",
      "  [191] g__RB41              (ID:   715) | Bin:   4\n",
      "  [192] g__RCP2-54           (ID:   148) | Bin:   2\n",
      "  [193] g__cvE6              (ID:   378) | Bin:   3\n",
      "  [194] <eos>                (ID:     3) | Bin:   0\n",
      "Metadata Conditions:\n",
      "  - Sample_Site: Code=0 -> 'BALF' (Org: 'BALF') ✅\n",
      "\n",
      "==================== Longest Sample (Len=512) (Index: 35173, ID: SRR7232633) ====================\n",
      "Total Length (with pad): 512\n",
      "Actual Length (tokens):  512\n",
      "Structure Check:\n",
      "  - Starts with BOS? True (ID: 2)\n",
      "  - Ends with EOS?   True (ID: 3, Found: 3)\n",
      "Token Details (Taxon ID | Bin ID):\n",
      "  --- Start ---\n",
      "  [  0] <bos>                (ID:     2) | Bin:   0\n",
      "  [  1] g__Oceanobacillus    (ID:   717) | Bin:  51\n",
      "  [  2] g__Pseudogracilibacillus (ID:   667) | Bin:  50\n",
      "  [  3] g__Moraxella         (ID:   202) | Bin:  50\n",
      "  [  4] g__Streptococcus     (ID:   483) | Bin:  50\n",
      "  ... ...\n",
      "  --- End ---\n",
      "  [507] g__Succinivibrio     (ID:   179) | Bin:   4\n",
      "  [508] g__Gordonia          (ID:   408) | Bin:   4\n",
      "  [509] g__Hungatella        (ID:   205) | Bin:   5\n",
      "  [510] g__JGI_0001001-H03   (ID:    54) | Bin:   3\n",
      "  [511] <eos>                (ID:     3) | Bin:   0\n",
      "Metadata Conditions:\n",
      "  - Sample_Site: Code=4 -> 'Nasopharynx' (Org: 'Nasopharynx') ✅\n",
      "\n",
      "==================== Shortest Sample (Len=4) (Index: 1514, ID: ERR689094) ====================\n",
      "Total Length (with pad): 512\n",
      "Actual Length (tokens):  4\n",
      "Structure Check:\n",
      "  - Starts with BOS? True (ID: 2)\n",
      "  - Ends with EOS?   True (ID: 3, Found: 3)\n",
      "Token Details (Taxon ID | Bin ID):\n",
      "  --- Start ---\n",
      "  [  0] <bos>                (ID:     2) | Bin:   0\n",
      "  [  1] g__Pseudomonas       (ID:   398) | Bin:  51\n",
      "  [  2] g__Streptococcus     (ID:   483) | Bin:   2\n",
      "  [  3] <eos>                (ID:     3) | Bin:   0\n",
      "  --- End ---\n",
      "  [  0] <bos>                (ID:     2) | Bin:   0\n",
      "  [  1] g__Pseudomonas       (ID:   398) | Bin:  51\n",
      "  [  2] g__Streptococcus     (ID:   483) | Bin:   2\n",
      "  [  3] <eos>                (ID:     3) | Bin:   0\n",
      "Metadata Conditions:\n",
      "  - Sample_Site: Code=7 -> 'Sputum' (Org: 'Sputum') ✅\n"
     ]
    }
   ],
   "source": [
    "def inspect_sample(idx, corpus, label=\"Sample\"):\n",
    "    sample_id = corpus.sample_ids[idx]\n",
    "    input_ids = corpus.input_ids[idx]\n",
    "    value_ids = corpus.value_ids[idx]\n",
    "    cond_ids = corpus.condition_ids[idx]\n",
    "    \n",
    "    # 获取实际长度（去除 padding）\n",
    "    # padding token id 通常是 0 或者 tokenizer.pad_token_id\n",
    "    # 这里我们通过 attention mask 或者直接找 pad token\n",
    "    pad_token_id = corpus.tokenizer.pad_token_id\n",
    "    if pad_token_id is None: pad_token_id = 0\n",
    "    \n",
    "    non_pad_mask = (input_ids != pad_token_id)\n",
    "    actual_len = non_pad_mask.sum().item()\n",
    "    \n",
    "    print(f\"\\n{'='*20} {label} (Index: {idx}, ID: {sample_id}) {'='*20}\")\n",
    "    print(f\"Total Length (with pad): {len(input_ids)}\")\n",
    "    print(f\"Actual Length (tokens):  {actual_len}\")\n",
    "    \n",
    "    # 1. BOS/EOS Check\n",
    "    bos_id = corpus.tokenizer.bos_token_id\n",
    "    eos_id = corpus.tokenizer.eos_token_id\n",
    "    \n",
    "    has_bos = (input_ids[0].item() == bos_id)\n",
    "    # EOS 应该在 actual_len - 1 的位置 (0-based index)\n",
    "    # 如果样本被截断 (actual_len == max_len)，可能没有 EOS？或者 corpus 处理时保留了？\n",
    "    # vCross 代码逻辑：input_ids[:self.max_len-1] + [input_ids[-1]] \n",
    "    # 如果 input_ids[-1] 原本就是 eos，那么截断后最后一个也是 eos。\n",
    "    # 如果原序列超长，最后一个 token 强制变为 eos 吗？\n",
    "    # vCross: sent = ['<bos>'] + taxa + ['<eos>']\n",
    "    # Truncate: input_ids[:max_len-1] + [input_ids[-1]] -> 这里 input_ids[-1] 是 eos_id。\n",
    "    # 所以理论上最后一个有效 token 必须是 EOS。\n",
    "    last_token = input_ids[actual_len-1].item()\n",
    "    has_eos = (last_token == eos_id)\n",
    "    \n",
    "    print(f\"Structure Check:\")\n",
    "    print(f\"  - Starts with BOS? {has_bos} (ID: {bos_id})\")\n",
    "    print(f\"  - Ends with EOS?   {has_eos} (ID: {eos_id}, Found: {last_token})\")\n",
    "    \n",
    "    # 2. Token-Level Detail (Show first 5 and last 5 valid tokens)\n",
    "    print(f\"Token Details (Taxon ID | Bin ID):\")\n",
    "    \n",
    "    # Helper to print token\n",
    "    def print_tok(i):\n",
    "        tid = input_ids[i].item()\n",
    "        vid = value_ids[i].item()\n",
    "        try:\n",
    "            # 尝试多种解码方式\n",
    "            if hasattr(corpus.tokenizer.vocab, 'lookup_token'):\n",
    "                token_str = corpus.tokenizer.vocab.lookup_token(tid)\n",
    "            elif hasattr(corpus.tokenizer.vocab, 'itos'):\n",
    "                token_str = corpus.tokenizer.vocab.itos[tid]\n",
    "            elif hasattr(corpus.tokenizer, 'decode'):\n",
    "                token_str = corpus.tokenizer.decode([tid])\n",
    "            else:\n",
    "                token_str = str(tid)\n",
    "        except:\n",
    "            token_str = \"???\"\n",
    "        print(f\"  [{i:3d}] {token_str:<20} (ID: {tid:5d}) | Bin: {vid:3d}\")\n",
    "\n",
    "    print(\"  --- Start ---\")\n",
    "    for i in range(min(5, actual_len)):\n",
    "        print_tok(i)\n",
    "        \n",
    "    if actual_len > 10:\n",
    "        print(\"  ... ...\")\n",
    "        \n",
    "    print(\"  --- End ---\")\n",
    "    for i in range(max(0, actual_len-5), actual_len):\n",
    "        print_tok(i)\n",
    "        \n",
    "    # 3. Metadata Check\n",
    "    print(f\"Metadata Conditions:\")\n",
    "    if corpus.use_meta_cols:\n",
    "        original_meta = corpus.metadata.loc[sample_id]\n",
    "        for k, col in enumerate(corpus.use_meta_cols):\n",
    "            code = cond_ids[k].item()\n",
    "            decoded = corpus.meta_encoders[col].inverse_transform([code])[0]\n",
    "            original = str(original_meta[col])\n",
    "            match = \"✅\" if (decoded == original or (original=='nan' and 'Unknown' in decoded)) else \"❌\"\n",
    "            print(f\"  - {col}: Code={code} -> '{decoded}' (Org: '{original}') {match}\")\n",
    "    else:\n",
    "        print(\"  (No metadata used)\")\n",
    "\n",
    "# 找出最长和最短样本\n",
    "lengths = []\n",
    "pad_id = corpus.tokenizer.pad_token_id if corpus.tokenizer.pad_token_id is not None else 0\n",
    "for i in range(len(corpus)):\n",
    "    l = (corpus.input_ids[i] != pad_id).sum().item()\n",
    "    lengths.append(l)\n",
    "\n",
    "max_idx = np.argmax(lengths)\n",
    "min_idx = np.argmin(lengths)\n",
    "\n",
    "# 展示\n",
    "inspect_sample(0, corpus, \"First Sample\")\n",
    "inspect_sample(1, corpus, \"Second Sample\")\n",
    "inspect_sample(2, corpus, \"Third Sample\")\n",
    "inspect_sample(max_idx, corpus, f\"Longest Sample (Len={lengths[max_idx]})\")\n",
    "inspect_sample(min_idx, corpus, f\"Shortest Sample (Len={lengths[min_idx]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a1854e",
   "metadata": {},
   "source": [
    "## 3. Value IDs (Binning) 分布统计\n",
    "如果不画图，我们需要详细的统计数据来确认分布的合理性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "444ca9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning Statistics (Total 5012551 tokens):\n",
      "- Min Bin ID: 2 (Expected: 1)\n",
      "- Max Bin ID: 51 (Expected: 51)\n",
      "- Coverage: 50/51 bins used\n",
      "\n",
      "Bin Distribution Percentiles:\n",
      "  0% (Min):   2.0\n",
      "  25% (Q1):   14.0\n",
      "  50% (Med):  26.0\n",
      "  75% (Q3):   39.0\n",
      "  100% (Max): 51.0\n",
      "\n",
      "Detailed Counts per Bin (First 10 & Last 10):\n",
      "2     110359\n",
      "3     102316\n",
      "4     101660\n",
      "5     100216\n",
      "6     100136\n",
      "7      98807\n",
      "8      97830\n",
      "9     102088\n",
      "10     99199\n",
      "11     99652\n",
      "12     99545\n",
      "13     99451\n",
      "14    101073\n",
      "15     97837\n",
      "16    104884\n",
      "17    100188\n",
      "18    101591\n",
      "19    100864\n",
      "20    101780\n",
      "21    100951\n",
      "22     98297\n",
      "23    106217\n",
      "24    101555\n",
      "25    101719\n",
      "26    103048\n",
      "27    100361\n",
      "28    102618\n",
      "29     97110\n",
      "30    107197\n",
      "31    101956\n",
      "32    101632\n",
      "33    102120\n",
      "34    102141\n",
      "35    101481\n",
      "36     96919\n",
      "37    107265\n",
      "38    101742\n",
      "39    101621\n",
      "40    101505\n",
      "41    101651\n",
      "42    101502\n",
      "43     95434\n",
      "44    106778\n",
      "45    100814\n",
      "46    100954\n",
      "47    100939\n",
      "48    100832\n",
      "49    100312\n",
      "50     56069\n",
      "51     90335\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Imbalance Check:\n",
      "- Least frequent bin count: 56069\n",
      "- Most frequent bin count:  110359\n",
      "- Ratio (Max/Min): 1.97\n"
     ]
    }
   ],
   "source": [
    "# 聚合所有样本的 Value IDs (排除 0 padding)\n",
    "all_values = corpus.value_ids.flatten()\n",
    "non_zero_values = all_values[all_values > 0].numpy()\n",
    "\n",
    "# 计算各 Bin 的频次\n",
    "bin_counts = pd.Series(non_zero_values).value_counts().sort_index()\n",
    "\n",
    "print(f\"Binning Statistics (Total {len(non_zero_values)} tokens):\")\n",
    "print(f\"- Min Bin ID: {non_zero_values.min()} (Expected: 1)\")\n",
    "print(f\"- Max Bin ID: {non_zero_values.max()} (Expected: {corpus.num_bins})\")\n",
    "print(f\"- Coverage: {len(bin_counts)}/{corpus.num_bins} bins used\")\n",
    "\n",
    "# 展示分布的百分比 (Quantiles)\n",
    "print(\"\\nBin Distribution Percentiles:\")\n",
    "percentiles = np.percentile(non_zero_values, [0, 25, 50, 75, 100])\n",
    "print(f\"  0% (Min):   {percentiles[0]}\")\n",
    "print(f\"  25% (Q1):   {percentiles[1]}\")\n",
    "print(f\"  50% (Med):  {percentiles[2]}\")\n",
    "print(f\"  75% (Q3):   {percentiles[3]}\")\n",
    "print(f\"  100% (Max): {percentiles[4]}\")\n",
    "\n",
    "print(\"\\nDetailed Counts per Bin (First 10 & Last 10):\")\n",
    "print(bin_counts.head(51))\n",
    "\n",
    "\n",
    "# 检查是否存在极度不均衡（例如某个 Bin 只有几个 Token）\n",
    "min_count = bin_counts.min()\n",
    "max_count = bin_counts.max()\n",
    "print(f\"\\nImbalance Check:\")\n",
    "print(f\"- Least frequent bin count: {min_count}\")\n",
    "print(f\"- Most frequent bin count:  {max_count}\")\n",
    "print(f\"- Ratio (Max/Min): {max_count/min_count:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e3a7e",
   "metadata": {},
   "source": [
    "## 4. Metadata Condition IDs 检查 (Skipped)\n",
    "已集成在样本深度检查中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d19211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "575023e5",
   "metadata": {},
   "source": [
    "## 5. Ranking 逻辑验证\n",
    "验证 Input IDs 对应的 Token 序列，是否真的是按照 Value IDs (Bin) 的大小顺序排列的（大致单调递减）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f00a78f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Token Sequence (first 20): [359 476 370 374 712 748 564 345 703   4 301 405 540 634  65 526 721 484\n",
      " 677 399]\n",
      "Corresponding Bins (first 20):   [51 50 50 49 49 49 48 48 47 47 47 46 46 46 45 45 44 44 44 43]\n",
      "\n",
      "Are Bins monotonically non-increasing? False\n",
      "Note: Strict monotonicity might be broken slightly due to random noise in binning edge cases, but overall trend should be decreasing.\n",
      "Explanation: This is expected behavior with Seeded Random Noise Binning. Tied values (e.g. low counts) are randomly distributed across adjacent bins to preserve magnitude information.\n"
     ]
    }
   ],
   "source": [
    "input_ids = corpus.input_ids[idx]\n",
    "value_ids = corpus.value_ids[idx]\n",
    "\n",
    "# 过滤掉 padding 和 special tokens (0)\n",
    "valid_mask = (value_ids > 0)\n",
    "valid_bins = value_ids[valid_mask].numpy()\n",
    "valid_tokens = input_ids[valid_mask].numpy()\n",
    "\n",
    "print(f\"Valid Token Sequence (first 20): {valid_tokens[:20]}\")\n",
    "print(f\"Corresponding Bins (first 20):   {valid_bins[:20]}\")\n",
    "\n",
    "# 检查 Bin 是否大致递减\n",
    "# 注意：Binning 是基于数值的，且我们之前对样本进行了 sort_values(ascending=False)\n",
    "# 所以 Bin ID 应该也是总体呈现从大到小（允许局部相等）。\n",
    "is_sorted = np.all(valid_bins[:-1] >= valid_bins[1:])\n",
    "print(f\"\\nAre Bins monotonically non-increasing? {is_sorted}\")\n",
    "\n",
    "if not is_sorted:\n",
    "    print(\"Note: Strict monotonicity might be broken slightly due to random noise in binning edge cases, but overall trend should be decreasing.\")\n",
    "    print(\"Explanation: This is expected behavior with Seeded Random Noise Binning. Tied values (e.g. low counts) are randomly distributed across adjacent bins to preserve magnitude information.\")\n",
    "    # plt.plot(valid_bins)\n",
    "    # plt.title(\"Bin Values along Sequence Position\")\n",
    "    # plt.ylabel(\"Bin ID\")\n",
    "    # plt.xlabel(\"Token Position\")\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
